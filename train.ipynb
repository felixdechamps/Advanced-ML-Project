{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f38491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63593afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:46:11.547923: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-03 10:46:11.548419: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-03 10:46:11.615687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-03 10:46:13.399890: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-03 10:46:13.400356: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7146c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100\n",
    "\n",
    "def make_save_dir(dirname, experiment_name):\n",
    "    start_time = str(int(time.time())) + '-' + str(random.randrange(1000))\n",
    "    save_dir = os.path.join(dirname, experiment_name, start_time)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    return save_dir\n",
    "\n",
    "def get_filename_for_saving(save_dir):\n",
    "    return os.path.join(save_dir,\n",
    "            \"{val_loss:.3f}-{val_acc:.3f}-{epoch:03d}-{loss:.3f}-{acc:.3f}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ba6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "    \"conv_filter_length\": 16,\n",
    "    \"conv_num_filters_start\": 32,\n",
    "    \"conv_init\": \"he_normal\",\n",
    "    \"conv_activation\": \"relu\",\n",
    "    \"conv_dropout\": 0.2,\n",
    "    \"conv_num_skip\": 2,\n",
    "    \"conv_increase_channels_at\": 4,\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    \"train\": \"train.json\",\n",
    "    \"dev\": \"dev.json\",\n",
    "\n",
    "    \"generator\": True,\n",
    "\n",
    "    \"save_dir\": \"saved\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439e79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:01<00:00, 4216.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X) : \n",
      " 7676\n",
      "len(y) : \n",
      " 7676\n",
      "ecg_0 : \n",
      " [  72   83   93 ... -136 -133 -131]\n",
      "len(ecg_0) : 8960\n",
      "label_0 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_0) : 35\n",
      "ecg_1 : \n",
      " [-137 -167 -200 ...  -50  -51  -51]\n",
      "len(ecg_1) : 8960\n",
      "label_1 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_1) : 35\n",
      "ecg_2 : \n",
      " [620 780 914 ... 102 115 116]\n",
      "len(ecg_2) : 8960\n",
      "label_2 :\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "len(label_2) : 35\n",
      "ecg_3 : \n",
      " [ 96 116 128 ...  45  52  62]\n",
      "len(ecg_3) : 8960\n",
      "label_3 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_3) : 35\n",
      "ecg_4 : \n",
      " [702 837 986 ... 150 147 143]\n",
      "len(ecg_4) : 8960\n",
      "label_4 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_4) : 35\n",
      "ecg_5 : \n",
      " [ -986 -1188 -1396 ...   266   390   531]\n",
      "len(ecg_5) : 8960\n",
      "label_5 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_5) : 35\n",
      "ecg_6 : \n",
      " [ 19  17  12 ... 274 269 260]\n",
      "len(ecg_6) : 6656\n",
      "label_6 :\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "len(label_6) : 26\n",
      "ecg_7 : \n",
      " [132 163 198 ... -46 -43 -40]\n",
      "len(ecg_7) : 8960\n",
      "label_7 :\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "len(label_7) : 35\n",
      "ecg_8 : \n",
      " [-10 -12 -13 ... -61 -62 -62]\n",
      "len(ecg_8) : 8960\n",
      "label_8 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_8) : 35\n",
      "ecg_9 : \n",
      " [153 182 216 ...  89  88  88]\n",
      "len(ecg_9) : 17920\n",
      "label_9 :\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "len(label_9) : 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load.load_dataset(params['train'])\n",
    "ecgs, labels = train\n",
    "print(f\"len(X) : \\n {len(ecgs)}\")\n",
    "print(f\"len(y) : \\n {len(labels)}\")\n",
    "for i in range(10):\n",
    "    print(f\"ecg_{i} : \\n {ecgs[i]}\")\n",
    "    print(f\"len(ecg_{i}) : {len(ecgs[i])}\")\n",
    "    print(f\"label_{i} :\\n {labels[i]}\")\n",
    "    print(f\"len(label_{i}) : {len(labels[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489b6ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 4903.11it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dev set...\")\n",
    "dev = load.load_dataset(params['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a928d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building preprocessor...\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.int_to_class :  {0: 'A', 1: 'N', 2: 'O', 3: '~'}\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "Training size: 7676 examples.\n",
      "Dev size: 852 examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building preprocessor...\")\n",
    "preproc = load.Preproc(*train)\n",
    "print(\"Training size: \" + str(len(train[0])) + \" examples.\")\n",
    "print(\"Dev size: \" + str(len(dev[0])) + \" examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2762b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet1d import MyDataset, ResNet1D\n",
    "\n",
    "train_dataset = MyDataset(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f5e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc40354",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data[:5], labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1048e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  72,   83,   93, ..., -136, -133, -131],\n",
       "       shape=(8960,), dtype=int16),\n",
       " array([-137, -167, -200, ...,  -50,  -51,  -51],\n",
       "       shape=(8960,), dtype=int16),\n",
       " array([620, 780, 914, ..., 102, 115, 116], shape=(8960,), dtype=int16),\n",
       " array([ 96, 116, 128, ...,  45,  52,  62], shape=(8960,), dtype=int16),\n",
       " array([702, 837, 986, ..., 150, 147, 143], shape=(8960,), dtype=int16)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0959c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa3aa631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for el in y : \n",
    "    print(len(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79d88c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.27332893],\n",
       "        [ 0.31991875],\n",
       "        [ 0.36227313],\n",
       "        ...,\n",
       "        [-0.60764205],\n",
       "        [-0.5949358 ],\n",
       "        [-0.5864649 ]],\n",
       "\n",
       "       [[-0.6118775 ],\n",
       "        [-0.7389406 ],\n",
       "        [-0.8787101 ],\n",
       "        ...,\n",
       "        [-0.24339443],\n",
       "        [-0.24762988],\n",
       "        [-0.24762988]],\n",
       "\n",
       "       [[ 2.5943487 ],\n",
       "        [ 3.2720187 ],\n",
       "        [ 3.8395672 ],\n",
       "        ...,\n",
       "        [ 0.40039206],\n",
       "        [ 0.45545274],\n",
       "        [ 0.4596882 ]],\n",
       "\n",
       "       [[ 0.37497944],\n",
       "        [ 0.4596882 ],\n",
       "        [ 0.5105134 ],\n",
       "        ...,\n",
       "        [ 0.15897211],\n",
       "        [ 0.18862018],\n",
       "        [ 0.23097456]],\n",
       "\n",
       "       [[ 2.9416544 ],\n",
       "        [ 3.5134387 ],\n",
       "        [ 4.144519  ],\n",
       "        ...,\n",
       "        [ 0.60369307],\n",
       "        [ 0.5909867 ],\n",
       "        [ 0.574045  ]]], shape=(5, 8960, 1), dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.process_x(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd75551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y in process_y before pad : \n",
      " [['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']]\n",
      "y in process_y after pad : \n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "y.shape in process_y after pad :  (5, 35)\n",
      "y in process_y after keras : \n",
      " tensor([[[0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0],\n",
      "         [0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 0, 0]]])\n",
      "y.shape in process_y after keras : \n",
      " torch.Size([5, 35, 4])\n"
     ]
    }
   ],
   "source": [
    "y_prep = preproc.process_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "707d79fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prep[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "694e248c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/resnet1d.py:26\u001b[39m, in \u001b[36mMyDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch.tensor(\u001b[38;5;28mself\u001b[39m.data[index], dtype=torch.float), \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse  # Module pour gérer les arguments passés en ligne de commande\n",
    "\n",
    "# Création du parseur d'arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Argument positionnel obligatoire : chemin vers un fichier de configuration\n",
    "parser.add_argument(\n",
    "    \"config_file\",\n",
    "    help=\"path to config file\"\n",
    ")\n",
    "\n",
    "# Argument optionnel : nom de l'expérience\n",
    "# Peut être fourni avec --experiment ou -e\n",
    "# Valeur par défaut : \"default\"\n",
    "parser.add_argument(\n",
    "    \"--experiment\",\n",
    "    \"-e\",\n",
    "    help=\"tag with experiment name\",\n",
    "    default=\"default\"\n",
    ")\n",
    "\n",
    "# Analyse des arguments fournis lors de l'exécution du script\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Après cette ligne :\n",
    "# args.config_file contient le chemin du fichier de configuration\n",
    "# args.experiment contient le nom de l'expérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "173a428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_subsample_lengths': [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], 'conv_filter_length': 16, 'conv_num_filters_start': 32, 'conv_init': 'he_normal', 'conv_activation': 'relu', 'conv_dropout': 0.2, 'conv_num_skip': 2, 'conv_increase_channels_at': 4, 'learning_rate': 0.001, 'batch_size': 32, 'train': 'train.json', 'dev': 'dev.json', 'generator': True, 'save_dir': 'saved', 'input_shape': [None, 1], 'num_categories': 4}\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "save_dir = make_save_dir(params['save_dir'], \"test_run\")\n",
    "\n",
    "util.save(preproc, save_dir)\n",
    "\n",
    "params.update({\n",
    "    \"input_shape\": [None, 1],\n",
    "    \"num_categories\": len(preproc.classes)\n",
    "})\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1759e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7548ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), hparams[\"learning_rate\"])\n",
    "\n",
    "    # The scheduler is used to change the learning rate every few \"n\" steps.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.8))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training stage, where we want to update the parameters.\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            images, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            images, labels = images.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "            # Flatten the images to a vector. This is done because the classifier expects a vector as input.\n",
    "            # Could also be done by reshaping the images in the dataset.\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            pred = model(images) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            scheduler.step() # Update the learning rate.\n",
    "\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                pred = model(images)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb145a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = \"logs\"\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "labled_train_loader = data_module.train_dataloader()\n",
    "labled_val_loader = data_module.val_dataloader()\n",
    "\n",
    "epochs = hparams.get('epochs', 4)\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "model = MyPytorchModel(hparams).to(device)\n",
    "train_model(model, labled_train_loader, labled_val_loader, loss_func, tb_logger, epochs=epochs, name=\"Default\")\n",
    "\n",
    "print()\n",
    "print(\"Finished training!\")\n",
    "print(\"How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\")\n",
    "print(f\"Training Acc: {model.getTestAcc(labled_train_loader)[1] * 100}%\")\n",
    "print(f\"Validation Acc: {model.getTestAcc(labled_val_loader)[1] * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
