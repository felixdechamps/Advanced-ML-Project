{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d798ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler, load_dataset, load_ecg\n",
    "from resnet1d import ResNet1D\n",
    "from mask import Mask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75c901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training stage, where we want to update the parameters.\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            ecgs, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            ecgs, labels = ecgs.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "\n",
    "            pred = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            # scheduler.step() # Update the learning rate.\n",
    "\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            #tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                pred = model(ecgs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                #tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "        \n",
    "        scheduler.step(np.mean(validation_loss))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cec1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(pruning_fraction: float = 0.2, pruning_layers_to_ignore: str = None, trained_model = None, current_mask: Mask = None) : \n",
    "    \"\"\"\n",
    "    A one iteration of pruning : returns the new updated mask after pruning.\n",
    "\n",
    "    trained_model : the original fully trained model.\n",
    "    pruning_fraction = The fraction of additional weights to prune from the network.\n",
    "    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\n",
    "    \"\"\"\n",
    "    current_mask = Mask.ones_like(trained_model).numpy() if current_mask is None else current_mask.numpy()\n",
    "\n",
    "    # Determine the number of weights that need to be pruned.\n",
    "    number_of_remaining_weights = np.sum([np.sum(v) for v in current_mask.values()])\n",
    "    number_of_weights_to_prune = np.ceil(pruning_fraction * number_of_remaining_weights).astype(int)\n",
    "\n",
    "    # Determine which layers can be pruned.\n",
    "    prunable_tensors = set(trained_model.prunable_layer_names)\n",
    "    if pruning_layers_to_ignore:\n",
    "        prunable_tensors -= set(pruning_layers_to_ignore.split(','))\n",
    "    print(\"prunable_tensors : \\n\", prunable_tensors)\n",
    "    # Get the model weights.\n",
    "    weights = {k: v.clone().cpu().detach().numpy()\n",
    "                for k, v in trained_model.state_dict().items()\n",
    "                if k in prunable_tensors}\n",
    "\n",
    "    # Create a vector of all the unpruned weights in the model.\n",
    "    weight_vector = np.concatenate([v[current_mask[k] == 1] for k, v in weights.items()])\n",
    "    threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "\n",
    "    new_mask = Mask({k: np.where(np.abs(v) > threshold, current_mask[k], np.zeros_like(v))\n",
    "                        for k, v in weights.items()})\n",
    "    for k in current_mask:\n",
    "        if k not in new_mask: # if this weight was already pruned add it to the new mask\n",
    "            new_mask[k] = current_mask[k]\n",
    "\n",
    "    return new_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def23b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedModel(ResNet1D): # Remplacer Model par ResNet1D \n",
    "    @staticmethod\n",
    "    def to_mask_name(name):\n",
    "        return 'mask_' + name.replace('.', '___')\n",
    "\n",
    "    def __init__(self, model: ResNet1D, mask: Mask):\n",
    "        if isinstance(model, PrunedModel): raise ValueError('Cannot nest pruned models.')\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for k in self.model.prunable_layer_names:\n",
    "            if k not in mask: raise ValueError('Missing mask value {}.'.format(k))\n",
    "            if not np.array_equal(mask[k].shape, np.array(self.model.state_dict()[k].shape)):\n",
    "                raise ValueError('Incorrect mask shape {} for tensor {}.'.format(mask[k].shape, k))\n",
    "\n",
    "        for k in mask:\n",
    "            if k not in self.model.prunable_layer_names:\n",
    "                raise ValueError('Key {} found in mask but is not a valid model tensor.'.format(k))\n",
    "\n",
    "        for k, v in mask.items(): self.register_buffer(PrunedModel.to_mask_name(k), v.float())\n",
    "        self._apply_mask()\n",
    "\n",
    "    def _apply_mask(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if hasattr(self, PrunedModel.to_mask_name(name)):\n",
    "                param.data *= getattr(self, PrunedModel.to_mask_name(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._apply_mask()\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    @property\n",
    "    def prunable_layer_names(self):\n",
    "        return self.model.prunable_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def output_layer_names(self):\n",
    "    #     return self.model.output_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def loss_criterion(self):\n",
    "    #     return self.model.loss_criterion\n",
    "\n",
    "    # def save(self, save_location, save_step):\n",
    "    #     self.model.save(save_location, save_step)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def default_hparams(): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def is_valid_model_name(model_name): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def get_model_from_name(model_name, outputs, initializer): raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d19cf992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "                  \"p_init\" : 30,\n",
    "                  \"target_reduction_factor\" : 120, \n",
    "                  \"alpha\" : 1.1,\n",
    "                  \"pruning_layers_to_ignore\" : None\n",
    "                  }\n",
    "\n",
    "\n",
    "def run_lth_ecg(pruning_params, network, train_loader, val_loader, loss_func) : \n",
    "    # Randomly initialize the given DL network D. (quelle initialisation ? Hannun et al. -> \"He normal\")\n",
    "    pruning_fraction = pruning_params[\"p_init\"]/100\n",
    "    current_mask = Mask.ones_like(network).numpy()\n",
    "    initial_weights_number = np.sum([np.sum(v) for v in current_mask.values()]) # eta \n",
    "    print(f\"eta = {initial_weights_number:.2e}\")\n",
    "    # current_model = network\n",
    "    initial_untrained_model = network\n",
    "    \n",
    "    remaining_weights_number = initial_weights_number\n",
    "\n",
    "    D = network #current_network\n",
    "    step = 0\n",
    "    while (initial_weights_number/remaining_weights_number) < pruning_params[\"target_reduction_factor\"]:\n",
    "        print(\"=\"*60,f\"STEP : {step}\")\n",
    "        print(f\"remaining_weights_number = {remaining_weights_number:.2e}\")\n",
    "        print(\"current reduction factor = \", np.round(initial_weights_number/remaining_weights_number, 2))\n",
    "        print(\"=\"*60, \"\\n\")\n",
    "        # Train the DL network with the given data x.\n",
    "        D = train_model(D, train_loader, val_loader, loss_func, name = \"lth_ecg\", tb_logger=None)\n",
    "\n",
    "        # Prune p_init% of weights which are of least magnitude\n",
    "        new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], D)\n",
    "\n",
    "        D_sparse = PrunedModel(D,new_mask) \n",
    "\n",
    "        pruning_fraction = pruning_fraction**(1/alpha) # alpha = 1.1\n",
    "\n",
    "        # reset unpruned weights to their initial random values and D = D_sparse\n",
    "        D = PrunedModel(model=initial_untrained_model, mask=new_mask)\n",
    "        \n",
    "        remaining_weights_number = np.sum([np.sum(v) for v in new_mask.values()])\n",
    "\n",
    "        current_mask = new_mask\n",
    "        step+=1\n",
    "\n",
    "    return D_sparse, current_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ce2980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:43<00:00, 176.13it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load_dataset(\"train.json\",256)\n",
    "train_ecgs, train_labels = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1604e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduciton of size to improve training time\n",
    "train_ecgs, train_labels = train_ecgs[:1000], train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdee2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:04<00:00, 206.53it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dev set...\")\n",
    "val_ecgs,val_labels = load_dataset(\"dev.json\",256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8108fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduciton of size to improve training time\n",
    "val_ecgs, val_labels = val_ecgs[:100], val_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2be3a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN :  7.1447935  STD :  227.54382\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "MEAN :  6.4466558  STD :  234.6957\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ECGDataset(train_ecgs, train_labels)\n",
    "val_dataset = ECGDataset(val_ecgs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7991c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri du dataset par longueur pour minimiser le padding...\n",
      "Tri du dataset par longueur pour minimiser le padding...\n"
     ]
    }
   ],
   "source": [
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21214058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=4, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose = False\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80d0f2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 1.05e+07\n",
      "============================================================ STEP : 0\n",
      "remaining_weights_number = 1.05e+07 1.0454528e+07\n",
      "current reduction factor =  1.0\n",
      "============================================================ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/10]:   3%|█▌                                              | 1/32 [00:11<06:07, 11.87s/it, curr_train_loss=1.32948554, lr=0.00100000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sparse_model, final_mask = \u001b[43mrun_lth_ecg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_lth_ecg\u001b[39m\u001b[34m(pruning_params, network, train_loader, val_loader, loss_func)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Train the DL network with the given data x.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m D = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlth_ecg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Prune p_init% of weights which are of least magnitude\u001b[39;00m\n\u001b[32m     31\u001b[39m new_mask = prune(pruning_fraction, pruning_params[\u001b[33m\"\u001b[39m\u001b[33mpruning_layers_to_ignore\u001b[39m\u001b[33m\"\u001b[39m], D)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, loss_func, tb_logger, epochs, name)\u001b[39m\n\u001b[32m     29\u001b[39m ecgs, labels = batch \u001b[38;5;66;03m# Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\u001b[39;00m\n\u001b[32m     30\u001b[39m ecgs, labels = ecgs.to(device), labels.to(device) \u001b[38;5;66;03m# Send the data to the device (GPU or CPU) - it has to be the same device as the model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Stage 1: Forward().\u001b[39;00m\n\u001b[32m     34\u001b[39m loss = loss_func(pred, labels) \u001b[38;5;66;03m# Compute the loss over the predictions and the ground truth.\u001b[39;00m\n\u001b[32m     35\u001b[39m loss.backward()  \u001b[38;5;66;03m# Stage 2: Backward().\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/resnet1d.py:289\u001b[39m, in \u001b[36mResNet1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mi_block: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, in_channels: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m, out_channels: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m, downsample: \u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[33m'\u001b[39m.format(i_block, net.in_channels, net.out_channels, net.downsample))\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m out = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mprint\u001b[39m(out.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/resnet1d.py:160\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    158\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu2(out)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_do:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv2(out)\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# if downsample, also downsample identity\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sparse_model, final_mask = run_lth_ecg(pruning_params,model, train_loader, val_loader, loss_func) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
