{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d798ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler, load_dataset, load_ecg\n",
    "from resnet1d import ResNet1D\n",
    "from mask import Mask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training stage, where we want to update the parameters.\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            ecgs, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            ecgs, labels = ecgs.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "\n",
    "            pred = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            # scheduler.step() # Update the learning rate.\n",
    "\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            #tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                pred = model(ecgs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                #tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "        \n",
    "        scheduler.step(np.mean(validation_loss))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cec1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(pruning_fraction: float = 0.2, pruning_layers_to_ignore: str = None, trained_model = None, current_mask: Mask = None) : \n",
    "    \"\"\"\n",
    "    A one iteration of pruning : returns the new updated mask after pruning.\n",
    "\n",
    "    trained_model : the original fully trained model.\n",
    "    pruning_fraction = The fraction of additional weights to prune from the network.\n",
    "    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\n",
    "    \"\"\"\n",
    "    current_mask = Mask.ones_like(trained_model).numpy() if current_mask is None else current_mask.numpy()\n",
    "\n",
    "    # Determine the number of weights that need to be pruned.\n",
    "    number_of_remaining_weights = np.sum([np.sum(v) for v in current_mask.values()])\n",
    "    number_of_weights_to_prune = np.ceil(pruning_fraction * number_of_remaining_weights).astype(int)\n",
    "\n",
    "    # Determine which layers can be pruned.\n",
    "    prunable_tensors = set(trained_model.prunable_layer_names)\n",
    "    if pruning_layers_to_ignore:\n",
    "        prunable_tensors -= set(pruning_layers_to_ignore.split(','))\n",
    "    print(\"prunable_tensors : \\n\", prunable_tensors)\n",
    "    # Get the model weights.\n",
    "    weights = {k: v.clone().cpu().detach().numpy()\n",
    "                for k, v in trained_model.state_dict().items()\n",
    "                if k in prunable_tensors}\n",
    "\n",
    "    # Create a vector of all the unpruned weights in the model.\n",
    "    weight_vector = np.concatenate([v[current_mask[k] == 1] for k, v in weights.items()])\n",
    "    threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "\n",
    "    new_mask = Mask({k: np.where(np.abs(v) > threshold, current_mask[k], np.zeros_like(v))\n",
    "                        for k, v in weights.items()})\n",
    "    for k in current_mask:\n",
    "        if k not in new_mask: # if this weight was already pruned add it to the new mask\n",
    "            new_mask[k] = current_mask[k]\n",
    "\n",
    "    return new_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "def23b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedModel(nn.Module): # Remplacer Model par ResNet1D \n",
    "    @staticmethod\n",
    "    def to_mask_name(name):\n",
    "        return 'mask_' + name.replace('.', '___')\n",
    "\n",
    "    def __init__(self, model: ResNet1D, mask: Mask):\n",
    "        if isinstance(model, PrunedModel): raise ValueError('Cannot nest pruned models.')\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for k in self.model.prunable_layer_names:\n",
    "            if k not in mask: raise ValueError('Missing mask value {}.'.format(k))\n",
    "            if not np.array_equal(mask[k].shape, np.array(self.model.state_dict()[k].shape)):\n",
    "                raise ValueError('Incorrect mask shape {} for tensor {}.'.format(mask[k].shape, k))\n",
    "\n",
    "        for k in mask:\n",
    "            if k not in self.model.prunable_layer_names:\n",
    "                raise ValueError('Key {} found in mask but is not a valid model tensor.'.format(k))\n",
    "\n",
    "        # for k, v in mask.items(): self.register_buffer(PrunedModel.to_mask_name(k), v.float())\n",
    "        # self._apply_mask()\n",
    "        device = next(model.parameters()).device \n",
    "\n",
    "        for k, v in mask.items(): \n",
    "            # On envoie le masque sur le même device que le modèle AVANT de l'enregistrer\n",
    "            self.register_buffer(PrunedModel.to_mask_name(k), v.float().to(device))\n",
    "            \n",
    "        self._apply_mask()\n",
    "\n",
    "    def _apply_mask(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if hasattr(self, PrunedModel.to_mask_name(name)):\n",
    "                param.data *= getattr(self, PrunedModel.to_mask_name(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._apply_mask()\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    @property\n",
    "    def prunable_layer_names(self):\n",
    "        return self.model.prunable_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def output_layer_names(self):\n",
    "    #     return self.model.output_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def loss_criterion(self):\n",
    "    #     return self.model.loss_criterion\n",
    "\n",
    "    # def save(self, save_location, save_step):\n",
    "    #     self.model.save(save_location, save_step)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def default_hparams(): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def is_valid_model_name(model_name): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def get_model_from_name(model_name, outputs, initializer): raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d19cf992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "                  \"p_init\" : 30,\n",
    "                  \"target_reduction_factor\" : 120, \n",
    "                  \"alpha\" : 1.1,\n",
    "                  \"pruning_layers_to_ignore\" : None\n",
    "                  }\n",
    "\n",
    "\n",
    "def run_lth_ecg(pruning_params, network, train_loader, val_loader, loss_func) : \n",
    "    # Randomly initialize the given DL network D. (quelle initialisation ? Hannun et al. -> \"He normal\")\n",
    "    pruning_fraction = pruning_params[\"p_init\"]/100\n",
    "    current_mask = Mask.ones_like(network).numpy()\n",
    "    initial_weights_number = np.sum([np.sum(v) for v in current_mask.values()]) # eta \n",
    "    print(f\"eta = {initial_weights_number:.2e}\")\n",
    "    # current_model = network\n",
    "    initial_untrained_model = copy.deepcopy(network)\n",
    "    \n",
    "    remaining_weights_number = initial_weights_number\n",
    "\n",
    "    D = copy.deepcopy(network) #current_network\n",
    "    step = 0\n",
    "    while (initial_weights_number/remaining_weights_number) < pruning_params[\"target_reduction_factor\"]:\n",
    "        print(\"=\"*60,f\"STEP : {step}\")\n",
    "        print(f\"remaining_weights_number = {remaining_weights_number:.2e}\")\n",
    "        print(\"current reduction factor = \", np.round(initial_weights_number/remaining_weights_number, 2))\n",
    "        print(\"=\"*60, \"\\n\")\n",
    "        # Train the DL network with the given data x.\n",
    "        D = train_model(D, train_loader, val_loader, loss_func, name = \"lth_ecg\", epochs=1,tb_logger=None)\n",
    "\n",
    "        # Prune p_init% of weights which are of least magnitude\n",
    "        new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], D)\n",
    "\n",
    "        #D_sparse = PrunedModel(D,new_mask) \n",
    "\n",
    "        pruning_fraction = pruning_fraction**(1/pruning_params['alpha']) # alpha = 1.1\n",
    "\n",
    "        # reset unpruned weights to their initial random values and D = D_sparse\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=new_mask).to(device)\n",
    "        \n",
    "        # remaining_weights_number = # On utilise la somme native de Python, et .item() pour extraire la valeur du tenseur\n",
    "        remaining_weights_number = sum(v.sum().item() for v in new_mask.values())\n",
    "\n",
    "        current_mask = new_mask\n",
    "        step+=1\n",
    "\n",
    "    return current_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ce2980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:01<00:00, 4233.13it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load_dataset(\"train.json\",256)\n",
    "train_ecgs, train_labels = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1604e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduciton of size to improve training time\n",
    "train_ecgs, train_labels = train_ecgs[:1000], train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdee2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 4257.38it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dev set...\")\n",
    "val_ecgs,val_labels = load_dataset(\"dev.json\",256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8108fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduciton of size to improve training time\n",
    "val_ecgs, val_labels = val_ecgs[:100], val_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2be3a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN :  7.4661856  STD :  236.10312\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "MEAN :  8.029898  STD :  242.35907\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ECGDataset(train_ecgs, train_labels)\n",
    "val_dataset = ECGDataset(val_ecgs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7991c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri du dataset par longueur pour minimiser le padding...\n",
      "Tri du dataset par longueur pour minimiser le padding...\n"
     ]
    }
   ],
   "source": [
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21214058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=4, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose = False\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d0f2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 1.05e+07\n",
      "============================================================ STEP : 0\n",
      "remaining_weights_number = 1.05e+07\n",
      "current reduction factor =  1.0\n",
      "============================================================ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/1]: 100%|██████████████████████████████████████████████| 240/240 [00:47<00:00,  5.01it/s, curr_train_loss=0.72173179, lr=0.00100000]\n",
      "Validation Epoch [1/1]: 100%|████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 15.54it/s, val_loss=0.69594120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prunable_tensors : \n",
      " {'basicblock_list.3.conv1.conv.weight', 'basicblock_list.1.conv1.conv.weight', 'basicblock_list.2.conv1.conv.weight', 'basicblock_list.3.conv2.conv.weight', 'basicblock_list.12.conv1.conv.weight', 'basicblock_list.10.conv2.conv.weight', 'basicblock_list.15.conv1.conv.weight', 'basicblock_list.5.conv1.conv.weight', 'basicblock_list.5.conv2.conv.weight', 'basicblock_list.13.conv1.conv.weight', 'basicblock_list.13.conv2.conv.weight', 'basicblock_list.15.conv2.conv.weight', 'first_block_conv.conv.weight', 'basicblock_list.7.conv2.conv.weight', 'basicblock_list.1.conv2.conv.weight', 'basicblock_list.11.conv1.conv.weight', 'basicblock_list.4.conv2.conv.weight', 'basicblock_list.0.conv1.conv.weight', 'basicblock_list.14.conv1.conv.weight', 'basicblock_list.4.conv1.conv.weight', 'basicblock_list.7.conv1.conv.weight', 'basicblock_list.8.conv2.conv.weight', 'basicblock_list.9.conv2.conv.weight', 'basicblock_list.0.conv2.conv.weight', 'basicblock_list.10.conv1.conv.weight', 'basicblock_list.12.conv2.conv.weight', 'basicblock_list.2.conv2.conv.weight', 'basicblock_list.11.conv2.conv.weight', 'dense.weight', 'basicblock_list.14.conv2.conv.weight', 'basicblock_list.6.conv2.conv.weight', 'basicblock_list.6.conv1.conv.weight', 'basicblock_list.8.conv1.conv.weight', 'basicblock_list.9.conv1.conv.weight'}\n",
      "============================================================ STEP : 1\n",
      "remaining_weights_number = 7.32e+06\n",
      "current reduction factor =  1.43\n",
      "============================================================ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/1]: 100%|██████████████████████████████████████████████| 240/240 [00:49<00:00,  4.85it/s, curr_train_loss=0.62704772, lr=0.00100000]\n",
      "Validation Epoch [1/1]: 100%|████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 15.01it/s, val_loss=0.65502224]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'first_block_conv.conv.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_mask = \u001b[43mrun_lth_ecg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_lth_ecg\u001b[39m\u001b[34m(pruning_params, network, train_loader, val_loader, loss_func)\u001b[39m\n\u001b[32m     28\u001b[39m D = train_model(D, train_loader, val_loader, loss_func, name = \u001b[33m\"\u001b[39m\u001b[33mlth_ecg\u001b[39m\u001b[33m\"\u001b[39m, epochs=\u001b[32m1\u001b[39m,tb_logger=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Prune p_init% of weights which are of least magnitude\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m new_mask = \u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_fraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruning_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpruning_layers_to_ignore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#D_sparse = PrunedModel(D,new_mask) \u001b[39;00m\n\u001b[32m     35\u001b[39m pruning_fraction = pruning_fraction**(\u001b[32m1\u001b[39m/pruning_params[\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m# alpha = 1.1\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mprune\u001b[39m\u001b[34m(pruning_fraction, pruning_layers_to_ignore, trained_model, current_mask)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprune\u001b[39m(pruning_fraction: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.2\u001b[39m, pruning_layers_to_ignore: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, trained_model = \u001b[38;5;28;01mNone\u001b[39;00m, current_mask: Mask = \u001b[38;5;28;01mNone\u001b[39;00m) : \n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    A one iteration of pruning : returns the new updated mask after pruning.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33;03m    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     current_mask = \u001b[43mMask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m)\u001b[49m.numpy() \u001b[38;5;28;01mif\u001b[39;00m current_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m current_mask.numpy()\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Determine the number of weights that need to be pruned.\u001b[39;00m\n\u001b[32m     12\u001b[39m     number_of_remaining_weights = np.sum([np.sum(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m current_mask.values()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/mask.py:36\u001b[39m, in \u001b[36mMask.ones_like\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     34\u001b[39m mask = Mask()\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m model.prunable_layer_names:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     mask[name] = torch.ones(\u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m.shape))\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[31mKeyError\u001b[39m: 'first_block_conv.conv.weight'"
     ]
    }
   ],
   "source": [
    "final_mask = run_lth_ecg(pruning_params, model, train_loader, val_loader, loss_func) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
