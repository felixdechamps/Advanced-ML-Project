{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a973793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 16:28:32.935415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler, load_dataset, load_ecg\n",
    "from resnet1d import ResNet1D\n",
    "from mask import Mask\n",
    "from plot_utils import plot_lth_progress, plot_layerwise_remaining_params\n",
    "from save_utils import save_checkpoint, load_checkpoint, push_checkpoint_to_git\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "325ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, prior, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=4, average=\"micro\").to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )                                           \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        accuracy_metric.reset()\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() \n",
    "            ecgs, labels = batch \n",
    "            ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(logits, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "            accuracy_metric.update(preds, labels)\n",
    "            current_train_accuracy = accuracy_metric.compute().item()\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      curr_train_accuracy=f\"{current_train_accuracy:.4f}\",\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "                                      )\n",
    "            # Update the tensorboard logger.\n",
    "            #tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        accuracy_metric.reset() # Reset pour calculer uniquement la val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(ecgs) # Sortie probable: (Batch, Classes, Time)\n",
    "                loss = loss_func(logits, labels)\n",
    "                # Update validation metrics\n",
    "                validation_loss.append(loss.item())\n",
    "                preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "                accuracy_metric.update(preds, labels)\n",
    "                current_val_accuracy = accuracy_metric.compute().item()\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(\n",
    "                    val_loss = \"{:.8f}\".format(np.mean(validation_loss)),\n",
    "                    f1_val=f\"{current_val_accuracy:.4f}\")\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                #tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "\n",
    "        if current_val_acccuracy > best_val_accuracy:\n",
    "            best_val_accuracy = current_val_acccuracy\n",
    "\n",
    "        scheduler.step(np.mean(validation_loss))\n",
    "\n",
    "        # --- LOGIQUE D'EARLY STOPPING ---\n",
    "        # Si le F1 dépasse 82.6% (0.826), on considère que le ticket a convergé avec une tolérance de 1%\n",
    "        # par rapport au 83.6% de l'article\n",
    "        if best_val_accuracy >= 0.826:\n",
    "            print(f\"\\n[Early Stopping] F1 Val ({best_val_accuracy:.4f}) >= 0.826. Fin de l'entraînement pour cette étape LTH.\")\n",
    "            break\n",
    "    \n",
    "    return model, best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c7c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(pruning_fraction: float = 0.2, pruning_layers_to_ignore: str = None, trained_model = None, current_mask: Mask = None) : \n",
    "    \"\"\"\n",
    "    A one iteration of pruning : returns the new updated mask after pruning.\n",
    "\n",
    "    trained_model : the original fully trained model.\n",
    "    pruning_fraction = The fraction of additional weights to prune from the network.\n",
    "    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\n",
    "    \"\"\"\n",
    "    current_mask = Mask.ones_like(trained_model).numpy() if current_mask is None else current_mask.numpy()\n",
    "\n",
    "    # Determine the number of weights that need to be pruned.\n",
    "    number_of_remaining_weights = np.sum([np.sum(v) for v in current_mask.values()])\n",
    "    number_of_weights_to_prune = np.ceil(pruning_fraction * number_of_remaining_weights).astype(int)\n",
    "\n",
    "    # Determine which layers can be pruned.\n",
    "    prunable_tensors = set(trained_model.prunable_layer_names)\n",
    "    if pruning_layers_to_ignore:\n",
    "        prunable_tensors -= set(pruning_layers_to_ignore.split(','))\n",
    "    \n",
    "    # Get the model weights.\n",
    "    weights = {k: v.clone().cpu().detach().numpy()\n",
    "                for k, v in trained_model.state_dict().items()\n",
    "                if k in prunable_tensors}\n",
    "\n",
    "    # Create a vector of all the unpruned weights in the model.\n",
    "    weight_vector = np.concatenate([v[current_mask[k] == 1] for k, v in weights.items()])\n",
    "    threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "\n",
    "    new_mask = Mask({k: np.where(np.abs(v) > threshold, current_mask[k], np.zeros_like(v))\n",
    "                        for k, v in weights.items()})\n",
    "    for k in current_mask:\n",
    "        if k not in new_mask: # if this weight was already pruned add it to the new mask\n",
    "            new_mask[k] = current_mask[k]\n",
    "\n",
    "    return new_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedModel(nn.Module): # Remplacer Model par ResNet1D \n",
    "    @staticmethod\n",
    "    def to_mask_name(name):\n",
    "        return 'mask_' + name.replace('.', '___')\n",
    "\n",
    "    def __init__(self, model: ResNet1D, mask: Mask):\n",
    "        if isinstance(model, PrunedModel): raise ValueError('Cannot nest pruned models.')\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for k in self.model.prunable_layer_names:\n",
    "            if k not in mask: raise ValueError('Missing mask value {}.'.format(k))\n",
    "            if not np.array_equal(mask[k].shape, np.array(self.model.state_dict()[k].shape)):\n",
    "                raise ValueError('Incorrect mask shape {} for tensor {}.'.format(mask[k].shape, k))\n",
    "\n",
    "        for k in mask:\n",
    "            if k not in self.model.prunable_layer_names:\n",
    "                raise ValueError('Key {} found in mask but is not a valid model tensor.'.format(k))\n",
    "\n",
    "        # for k, v in mask.items(): self.register_buffer(PrunedModel.to_mask_name(k), v.float())\n",
    "        # self._apply_mask()\n",
    "        device = next(model.parameters()).device \n",
    "\n",
    "        for k, v in mask.items(): \n",
    "            # On envoie le masque sur le même device que le modèle AVANT de l'enregistrer\n",
    "            self.register_buffer(PrunedModel.to_mask_name(k), v.float().to(device))\n",
    "            \n",
    "        self._apply_mask()\n",
    "\n",
    "    def _apply_mask(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if hasattr(self, PrunedModel.to_mask_name(name)):\n",
    "                param.data *= getattr(self, PrunedModel.to_mask_name(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._apply_mask()\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    @property\n",
    "    def prunable_layer_names(self):\n",
    "        return self.model.prunable_layer_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3267fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_priors(train_loader, device, num_classes=4, smooth=500):\n",
    "    \"\"\"Calcule la distribution des classes en gérant les labels séquentiels.\"\"\"\n",
    "    counts = torch.zeros(num_classes)\n",
    "    device = next(iter(train_loader))[1].device # Détection automatique du device\n",
    "    \n",
    "    for _, labels in train_loader:\n",
    "        # Si labels est (Batch, Séquence), on prend le premier label de chaque séquence\n",
    "        if labels.dim() > 1:\n",
    "            # labels[:, 0] récupère le premier label pour chaque échantillon du batch\n",
    "            labels = labels[:, 0]\n",
    "            \n",
    "        for l in labels:\n",
    "            counts[l.long().item()] += 1\n",
    "    \n",
    "    total = counts.sum() + num_classes\n",
    "    prior = (counts + smooth) / total\n",
    "    return prior.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8250091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "                  \"pruning_percentage\" : 30, # denoted as p in Frankle & Carbin\n",
    "                  \"nb_of_steps\" : 10, # denoted as n in Frankle & Carbin\n",
    "                  \"pruning_layers_to_ignore\" : None,\n",
    "                  \"weight_rewinding\": True\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f893f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterative_pruning(pruning_params, network, train_loader, val_loader, loss_func, resume=False) : \n",
    "    \n",
    "    prior = calculate_priors(train_loader,device,4,500).to(device)\n",
    "        \n",
    "\n",
    "    checkpoint_file = f\"iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}_checkpoint.pth\"\n",
    "    \n",
    "    # Randomly initialize the given DL network D. (quelle initialisation ? Hannun et al. -> \"He normal\")\n",
    "    pruning_percentage = pruning_params[\"pruning_percentage\"]\n",
    "    n = pruning_params['nb_of_steps']\n",
    "    current_mask = Mask.ones_like(network)\n",
    "    current_mask_np = current_mask.numpy()\n",
    "    initial_weights_number = np.sum([np.sum(v) for v in current_mask_np.values()]) # eta \n",
    "    \n",
    "    initial_untrained_model = copy.deepcopy(network)\n",
    "    \n",
    "    remaining_weights_number = initial_weights_number\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    # Listes pour l'historique\n",
    "    history_theta = []\n",
    "    history_acc = []\n",
    "    history_sparsity = []\n",
    "\n",
    "    ##########\n",
    "    if resume and os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file)\n",
    "        step = checkpoint['step']\n",
    "        pruning_percentage = checkpoint['pruning_percentage']\n",
    "        current_mask = checkpoint['current_mask']\n",
    "        history_theta = checkpoint['history_theta']\n",
    "        history_acc = checkpoint['history_acc']\n",
    "        history_sparsity = checkpoint['history_sparsity']\n",
    "        remaining_weights_number = sum(v.sum().item() for v in current_mask.values())\n",
    "        # On recharge les poids initiaux pour garantir le \"Winning Ticket\" \n",
    "        initial_untrained_model.load_state_dict(checkpoint['initial_weights'])\n",
    "        # On repart du modèle tel qu'il était avant le crash\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=current_mask).to(device)\n",
    "        print(f\"REPRISE à l'étape {step}\")   \n",
    "\n",
    "    else:\n",
    "        D = copy.deepcopy(network)\n",
    "    ##############\n",
    "    step_pruning_percentage = (pruning_percentage)**(1/n)\n",
    "    pruning_fraction = step_pruning_percentage/100\n",
    "\n",
    "    for i in range(n) : \n",
    "        \n",
    "        print(f\"\\n{'='*30} STEP {step} {'='*30}\")\n",
    "        print(f\"remaining_weights_number = {remaining_weights_number:.2e}\")\n",
    "        print(\"current reduction factor = \", np.round(initial_weights_number/remaining_weights_number, 2))\n",
    "        print(\"current pruning percentage = \", np.round(step*step_pruning_percentage,2), ' % ', ' over ', pruning_percentage, ' %')\n",
    "        print(\"pruning fraction = \", np.round(pruning_fraction,2))\n",
    "        print(\"model sparcity (percentage of weight that has been pruned): \", current_mask.sparsity )  \n",
    "        \n",
    "        # Train the DL network with the given data x.\n",
    "        D,final_accuracy = train_model(D, train_loader, val_loader, loss_func, name = \"lth_ecg\", tb_logger=None, prior = prior, epochs=20)\n",
    "\n",
    "        # 3. Archivage\n",
    "        history_theta.append(initial_weights_number/remaining_weights_number)\n",
    "        #history_acc.append(final_accuracy)\n",
    "        history_sparsity.append(current_mask.sparsity)\n",
    "\n",
    "        if isinstance(D, PrunedModel):\n",
    "            model_to_prune = D.model\n",
    "        else:\n",
    "            model_to_prune = D\n",
    "\n",
    "        # Prune p_init% of weights which are of least magnitude\n",
    "    \n",
    "        new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], model_to_prune, current_mask)\n",
    "\n",
    "        D_sparse = PrunedModel(model_to_prune, mask = new_mask)\n",
    "\n",
    "        D_sparse.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Sparse Model [step : {step}]')\n",
    "        accuracy_metric.reset() # Reset pour calculer uniquement la val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(ecgs) # Sortie probable: (Batch, Classes, Time)\n",
    "\n",
    "                # Update validation metrics\n",
    "                preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "                accuracy_metric.update(preds, labels)\n",
    "                current_val_accuracy = accuracy_metric.compute().item()\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(\n",
    "                    val_acc=f\"{current_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        history_acc.append(current_val_accuracy)\n",
    "\n",
    "\n",
    "        # Plots and push of checkpoints on github every 5 steps \n",
    "        if step % 5 == 0:\n",
    "            path = f'plots/iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}/'\n",
    "            plot_lth_progress(history_theta, history_acc, path)\n",
    "            plot_layerwise_remaining_params(D_sparse, current_mask, path)\n",
    "            # push_checkpoint_to_git(\n",
    "            #         filepaths=[checkpoint_file,\n",
    "            #         f\"plots/iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}_performance_plot.png\",\n",
    "            #         f\"plots/layerwise_remaining_params_plot_weight_rewinding_{str(pruning_params['weight_rewinding'])}.png\"],\n",
    "            #         branch_name=\"felix-v3\",  # Mets le nom de ta branche ici\n",
    "            #         commit_msg=f\"Auto save checkpoints and plots step {step-1}\"\n",
    "            #     )\n",
    "\n",
    "        # reset unpruned weights to their initial random values and D = D_sparse\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=new_mask).to(device)\n",
    "        \n",
    "        remaining_weights_number = sum(v.sum().item() for v in new_mask.values())\n",
    "       \n",
    "        current_mask = new_mask\n",
    "       \n",
    "        step+=1\n",
    "\n",
    "        # --- SAUVEGARDE DE SÉCURITÉ ---\n",
    "        checkpoint_state = {\n",
    "            'step': step,\n",
    "            'pruning_percentage': pruning_percentage,\n",
    "            'current_mask': current_mask,\n",
    "            'initial_weights': initial_untrained_model.state_dict(),\n",
    "            'reduction_factor': initial_weights_number / remaining_weights_number,\n",
    "            'history_theta': history_theta,\n",
    "            'history_acc': history_acc,\n",
    "            'history_sparsity': history_sparsity,\n",
    "            'layerwise_sparcity': current_mask.layerwise_sparsity(),\n",
    "            'layerwise_remaining_params': current_mask.layerwise_remaining_params()\n",
    "        }\n",
    "        save_checkpoint(checkpoint_state, filename=checkpoint_file)\n",
    "\n",
    "        print(\"=\"*60, \"\\n\")\n",
    "        \n",
    "\n",
    "    # Plot final à la fin de l'expérience\n",
    "    plot_lth_progress(history_theta, history_acc)\n",
    "\n",
    "    return current_mask, history_theta, history_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9c9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:01<00:00, 4809.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 5470.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN :  7.4661856  STD :  236.10312\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "MEAN :  8.029898  STD :  242.35907\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "Tri du dataset par longueur pour minimiser le padding...\n",
      "Tri du dataset par longueur pour minimiser le padding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load_dataset(\"train.json\",256)\n",
    "train_ecgs, train_labels = train\n",
    "# reduciton of size to improve training time\n",
    "# train_ecgs, train_labels = train_ecgs[:1000], train_labels[:1000]\n",
    "print(\"Loading dev set...\")\n",
    "val_ecgs,val_labels = load_dataset(\"dev.json\",256)\n",
    "# reduciton of size to improve training time\n",
    "# val_ecgs, val_labels = val_ecgs[:100], val_labels[:100]\n",
    "\n",
    "train_dataset = ECGDataset(train_ecgs, train_labels)\n",
    "val_dataset = ECGDataset(val_ecgs, val_labels)\n",
    "\n",
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f77e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_kaiming(m):\n",
    "    \"\"\"\n",
    "    Initialisation de Kaiming He (He Normal) pour les réseaux avec ReLU.\n",
    "    À utiliser avec model.apply(weights_init_kaiming).\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    # Pour les couches de Convolution (Conv1d) et Linéaires (Linear)\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        # kaiming_normal_ est la version \"He Normal\"\n",
    "        # mode='fan_out' est recommandé pour les ResNets (préserve la variance en passe avant)\n",
    "        # nonlinearity='relu' est crucial car ResNet utilise ReLU\n",
    "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        # Si la couche a un biais, on l'initialise à 0\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21393819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Initialisation Kaiming He appliquée.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=4, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose = False\n",
    "    ).to(device)\n",
    "\n",
    "# On applique l'initialisation explicite au réseau\n",
    "model.apply(weights_init_kaiming)\n",
    "print(\"Initialisation Kaiming He appliquée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9a3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 32, 100]             544\n",
      "   MyConv1dPadSame-2              [-1, 32, 100]               0\n",
      "       BatchNorm1d-3              [-1, 32, 100]              64\n",
      "              ReLU-4              [-1, 32, 100]               0\n",
      "            Conv1d-5              [-1, 32, 100]          16,416\n",
      "   MyConv1dPadSame-6              [-1, 32, 100]               0\n",
      "       BatchNorm1d-7              [-1, 32, 100]              64\n",
      "              ReLU-8              [-1, 32, 100]               0\n",
      "           Dropout-9              [-1, 32, 100]               0\n",
      "           Conv1d-10              [-1, 32, 100]          16,416\n",
      "  MyConv1dPadSame-11              [-1, 32, 100]               0\n",
      "       BasicBlock-12              [-1, 32, 100]               0\n",
      "      BatchNorm1d-13              [-1, 32, 100]              64\n",
      "             ReLU-14              [-1, 32, 100]               0\n",
      "           Conv1d-15               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-16               [-1, 32, 50]               0\n",
      "      BatchNorm1d-17               [-1, 32, 50]              64\n",
      "             ReLU-18               [-1, 32, 50]               0\n",
      "          Dropout-19               [-1, 32, 50]               0\n",
      "           Conv1d-20               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-21               [-1, 32, 50]               0\n",
      "        MaxPool1d-22               [-1, 32, 50]               0\n",
      "MyMaxPool1dPadSame-23               [-1, 32, 50]               0\n",
      "       BasicBlock-24               [-1, 32, 50]               0\n",
      "      BatchNorm1d-25               [-1, 32, 50]              64\n",
      "             ReLU-26               [-1, 32, 50]               0\n",
      "           Conv1d-27               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-28               [-1, 32, 50]               0\n",
      "      BatchNorm1d-29               [-1, 32, 50]              64\n",
      "             ReLU-30               [-1, 32, 50]               0\n",
      "          Dropout-31               [-1, 32, 50]               0\n",
      "           Conv1d-32               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-33               [-1, 32, 50]               0\n",
      "       BasicBlock-34               [-1, 32, 50]               0\n",
      "      BatchNorm1d-35               [-1, 32, 50]              64\n",
      "             ReLU-36               [-1, 32, 50]               0\n",
      "           Conv1d-37               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-38               [-1, 32, 25]               0\n",
      "      BatchNorm1d-39               [-1, 32, 25]              64\n",
      "             ReLU-40               [-1, 32, 25]               0\n",
      "          Dropout-41               [-1, 32, 25]               0\n",
      "           Conv1d-42               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-43               [-1, 32, 25]               0\n",
      "        MaxPool1d-44               [-1, 32, 25]               0\n",
      "MyMaxPool1dPadSame-45               [-1, 32, 25]               0\n",
      "       BasicBlock-46               [-1, 32, 25]               0\n",
      "      BatchNorm1d-47               [-1, 32, 25]              64\n",
      "             ReLU-48               [-1, 32, 25]               0\n",
      "           Conv1d-49               [-1, 64, 25]          32,832\n",
      "  MyConv1dPadSame-50               [-1, 64, 25]               0\n",
      "      BatchNorm1d-51               [-1, 64, 25]             128\n",
      "             ReLU-52               [-1, 64, 25]               0\n",
      "          Dropout-53               [-1, 64, 25]               0\n",
      "           Conv1d-54               [-1, 64, 25]          65,600\n",
      "  MyConv1dPadSame-55               [-1, 64, 25]               0\n",
      "       BasicBlock-56               [-1, 64, 25]               0\n",
      "      BatchNorm1d-57               [-1, 64, 25]             128\n",
      "             ReLU-58               [-1, 64, 25]               0\n",
      "           Conv1d-59               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-60               [-1, 64, 13]               0\n",
      "      BatchNorm1d-61               [-1, 64, 13]             128\n",
      "             ReLU-62               [-1, 64, 13]               0\n",
      "          Dropout-63               [-1, 64, 13]               0\n",
      "           Conv1d-64               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-65               [-1, 64, 13]               0\n",
      "        MaxPool1d-66               [-1, 64, 13]               0\n",
      "MyMaxPool1dPadSame-67               [-1, 64, 13]               0\n",
      "       BasicBlock-68               [-1, 64, 13]               0\n",
      "      BatchNorm1d-69               [-1, 64, 13]             128\n",
      "             ReLU-70               [-1, 64, 13]               0\n",
      "           Conv1d-71               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-72               [-1, 64, 13]               0\n",
      "      BatchNorm1d-73               [-1, 64, 13]             128\n",
      "             ReLU-74               [-1, 64, 13]               0\n",
      "          Dropout-75               [-1, 64, 13]               0\n",
      "           Conv1d-76               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-77               [-1, 64, 13]               0\n",
      "       BasicBlock-78               [-1, 64, 13]               0\n",
      "      BatchNorm1d-79               [-1, 64, 13]             128\n",
      "             ReLU-80               [-1, 64, 13]               0\n",
      "           Conv1d-81                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-82                [-1, 64, 7]               0\n",
      "      BatchNorm1d-83                [-1, 64, 7]             128\n",
      "             ReLU-84                [-1, 64, 7]               0\n",
      "          Dropout-85                [-1, 64, 7]               0\n",
      "           Conv1d-86                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-87                [-1, 64, 7]               0\n",
      "        MaxPool1d-88                [-1, 64, 7]               0\n",
      "MyMaxPool1dPadSame-89                [-1, 64, 7]               0\n",
      "       BasicBlock-90                [-1, 64, 7]               0\n",
      "      BatchNorm1d-91                [-1, 64, 7]             128\n",
      "             ReLU-92                [-1, 64, 7]               0\n",
      "           Conv1d-93               [-1, 128, 7]         131,200\n",
      "  MyConv1dPadSame-94               [-1, 128, 7]               0\n",
      "      BatchNorm1d-95               [-1, 128, 7]             256\n",
      "             ReLU-96               [-1, 128, 7]               0\n",
      "          Dropout-97               [-1, 128, 7]               0\n",
      "           Conv1d-98               [-1, 128, 7]         262,272\n",
      "  MyConv1dPadSame-99               [-1, 128, 7]               0\n",
      "      BasicBlock-100               [-1, 128, 7]               0\n",
      "     BatchNorm1d-101               [-1, 128, 7]             256\n",
      "            ReLU-102               [-1, 128, 7]               0\n",
      "          Conv1d-103               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-104               [-1, 128, 4]               0\n",
      "     BatchNorm1d-105               [-1, 128, 4]             256\n",
      "            ReLU-106               [-1, 128, 4]               0\n",
      "         Dropout-107               [-1, 128, 4]               0\n",
      "          Conv1d-108               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-109               [-1, 128, 4]               0\n",
      "       MaxPool1d-110               [-1, 128, 4]               0\n",
      "MyMaxPool1dPadSame-111               [-1, 128, 4]               0\n",
      "      BasicBlock-112               [-1, 128, 4]               0\n",
      "     BatchNorm1d-113               [-1, 128, 4]             256\n",
      "            ReLU-114               [-1, 128, 4]               0\n",
      "          Conv1d-115               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-116               [-1, 128, 4]               0\n",
      "     BatchNorm1d-117               [-1, 128, 4]             256\n",
      "            ReLU-118               [-1, 128, 4]               0\n",
      "         Dropout-119               [-1, 128, 4]               0\n",
      "          Conv1d-120               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-121               [-1, 128, 4]               0\n",
      "      BasicBlock-122               [-1, 128, 4]               0\n",
      "     BatchNorm1d-123               [-1, 128, 4]             256\n",
      "            ReLU-124               [-1, 128, 4]               0\n",
      "          Conv1d-125               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-126               [-1, 128, 2]               0\n",
      "     BatchNorm1d-127               [-1, 128, 2]             256\n",
      "            ReLU-128               [-1, 128, 2]               0\n",
      "         Dropout-129               [-1, 128, 2]               0\n",
      "          Conv1d-130               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-131               [-1, 128, 2]               0\n",
      "       MaxPool1d-132               [-1, 128, 2]               0\n",
      "MyMaxPool1dPadSame-133               [-1, 128, 2]               0\n",
      "      BasicBlock-134               [-1, 128, 2]               0\n",
      "     BatchNorm1d-135               [-1, 128, 2]             256\n",
      "            ReLU-136               [-1, 128, 2]               0\n",
      "          Conv1d-137               [-1, 256, 2]         524,544\n",
      " MyConv1dPadSame-138               [-1, 256, 2]               0\n",
      "     BatchNorm1d-139               [-1, 256, 2]             512\n",
      "            ReLU-140               [-1, 256, 2]               0\n",
      "         Dropout-141               [-1, 256, 2]               0\n",
      "          Conv1d-142               [-1, 256, 2]       1,048,832\n",
      " MyConv1dPadSame-143               [-1, 256, 2]               0\n",
      "      BasicBlock-144               [-1, 256, 2]               0\n",
      "     BatchNorm1d-145               [-1, 256, 2]             512\n",
      "            ReLU-146               [-1, 256, 2]               0\n",
      "          Conv1d-147               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-148               [-1, 256, 1]               0\n",
      "     BatchNorm1d-149               [-1, 256, 1]             512\n",
      "            ReLU-150               [-1, 256, 1]               0\n",
      "         Dropout-151               [-1, 256, 1]               0\n",
      "          Conv1d-152               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-153               [-1, 256, 1]               0\n",
      "       MaxPool1d-154               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-155               [-1, 256, 1]               0\n",
      "      BasicBlock-156               [-1, 256, 1]               0\n",
      "     BatchNorm1d-157               [-1, 256, 1]             512\n",
      "            ReLU-158               [-1, 256, 1]               0\n",
      "          Conv1d-159               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-160               [-1, 256, 1]               0\n",
      "     BatchNorm1d-161               [-1, 256, 1]             512\n",
      "            ReLU-162               [-1, 256, 1]               0\n",
      "         Dropout-163               [-1, 256, 1]               0\n",
      "          Conv1d-164               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-165               [-1, 256, 1]               0\n",
      "      BasicBlock-166               [-1, 256, 1]               0\n",
      "     BatchNorm1d-167               [-1, 256, 1]             512\n",
      "            ReLU-168               [-1, 256, 1]               0\n",
      "          Conv1d-169               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-170               [-1, 256, 1]               0\n",
      "     BatchNorm1d-171               [-1, 256, 1]             512\n",
      "            ReLU-172               [-1, 256, 1]               0\n",
      "         Dropout-173               [-1, 256, 1]               0\n",
      "          Conv1d-174               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-175               [-1, 256, 1]               0\n",
      "       MaxPool1d-176               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-177               [-1, 256, 1]               0\n",
      "      BasicBlock-178               [-1, 256, 1]               0\n",
      "     BatchNorm1d-179               [-1, 256, 1]             512\n",
      "            ReLU-180               [-1, 256, 1]               0\n",
      "          Conv1d-181                 [-1, 4, 1]           1,028\n",
      "================================================================\n",
      "Total params: 10,466,148\n",
      "Trainable params: 10,466,148\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.27\n",
      "Params size (MB): 39.93\n",
      "Estimated Total Size (MB): 41.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,100), device=device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e19e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== STEP 0 ==============================\n",
      "remaining_weights_number = 1.05e+07\n",
      "current reduction factor =  1.0\n",
      "current pruning percentage =  0.0  %   over  30  %\n",
      "pruning fraction =  0.01\n",
      "model sparcity (percentage of weight that has been pruned):  tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]:   2%|▍                  | 6/240 [00:01<00:58,  3.99it/s, curr_train_accuracy=0.1307, curr_train_loss=7.23341691, lr=0.00100000]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Detected more unique values in `target` than expected. Expected only 4 but found 5 in `target`. Found values: tensor([-100,    0,    1,    2,    3], device='cuda:0').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_mask, history_theta, history_acc = \u001b[43miterative_pruning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36miterative_pruning\u001b[39m\u001b[34m(pruning_params, network, train_loader, val_loader, loss_func, resume)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmodel sparcity (percentage of weight that has been pruned): \u001b[39m\u001b[33m\"\u001b[39m, current_mask.sparsity )  \n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Train the DL network with the given data x.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m D,final_accuracy = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlth_ecg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# 3. Archivage\u001b[39;00m\n\u001b[32m     61\u001b[39m history_theta.append(initial_weights_number/remaining_weights_number)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, loss_func, tb_logger, prior, epochs, name)\u001b[39m\n\u001b[32m     33\u001b[39m optimizer.step() \u001b[38;5;66;03m# Stage 3: Update the parameters.\u001b[39;00m\n\u001b[32m     35\u001b[39m preds = torch.argmax(logits, dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, S)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43maccuracy_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m current_train_accuracy = accuracy_metric.compute().item()\n\u001b[32m     38\u001b[39m training_loss.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torchmetrics/metric.py:559\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[32m    552\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    553\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m device corresponds to the device of the input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_on_cpu:\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m._move_list_states_to_cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torchmetrics/metric.py:549\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m._enable_grad):\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torchmetrics/classification/stat_scores.py:339\u001b[39m, in \u001b[36mMulticlassStatScores.update\u001b[39m\u001b[34m(self, preds, target)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.validate_args:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m preds, target = _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m.top_k)\n\u001b[32m    343\u001b[39m num_classes = \u001b[38;5;28mself\u001b[39m.num_classes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torchmetrics/functional/classification/stat_scores.py:322\u001b[39m, in \u001b[36m_multiclass_stat_scores_tensor_validation\u001b[39m\u001b[34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[39m\n\u001b[32m    320\u001b[39m num_unique_values = \u001b[38;5;28mlen\u001b[39m(torch.unique(t, dim=\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_unique_values > check_value:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    323\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but found\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. Found values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.unique(t,\u001b[38;5;250m \u001b[39mdim=\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Detected more unique values in `target` than expected. Expected only 4 but found 5 in `target`. Found values: tensor([-100,    0,    1,    2,    3], device='cuda:0')."
     ]
    }
   ],
   "source": [
    "final_mask, history_theta, history_acc = iterative_pruning(pruning_params, model, train_loader, val_loader, loss_func) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
