{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a973793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 16:15:03.839973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler, load_dataset, load_ecg\n",
    "from resnet1d import ResNet1D\n",
    "from mask import Mask\n",
    "from plot_utils import plot_lth_progress, plot_layerwise_remaining_params\n",
    "from save_utils import save_checkpoint, load_checkpoint, push_checkpoint_to_git\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, prior, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=4, average=\"micro\")\n",
    "    best_val_accuracy = 0.0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )                                           \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        accuracy_metric.reset()\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() \n",
    "            ecgs, labels = batch \n",
    "            ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(logits, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "            accuracy_metric.update(preds, targets)\n",
    "            current_train_accuracy = accuracy_metric.compute.item()\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      curr_train_accuracy=f\"{current_train_accuracy:.4f}\",\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "                                      )\n",
    "            # Update the tensorboard logger.\n",
    "            #tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        accuracy_metric.reset() # Reset pour calculer uniquement la val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(ecgs) # Sortie probable: (Batch, Classes, Time)\n",
    "                loss = loss_func(logits, labels)\n",
    "                # Update validation metrics\n",
    "                validation_loss.append(loss.item())\n",
    "                preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "                accuracy_metric.update(preds, targets)\n",
    "                current_val_accuracy = accuracy_metric.compute.item()\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(\n",
    "                    val_loss = \"{:.8f}\".format(np.mean(validation_loss)),\n",
    "                    f1_val=f\"{current_val_accuracy:.4f}\")\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                #tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "\n",
    "        if current_val_acccuracy > best_val_accuracy:\n",
    "            best_val_accuracy = current_val_acccuracy\n",
    "\n",
    "        scheduler.step(np.mean(validation_loss))\n",
    "\n",
    "        # --- LOGIQUE D'EARLY STOPPING ---\n",
    "        # Si le F1 dépasse 82.6% (0.826), on considère que le ticket a convergé avec une tolérance de 1%\n",
    "        # par rapport au 83.6% de l'article\n",
    "        if best_val_accuracy >= 0.826:\n",
    "            print(f\"\\n[Early Stopping] F1 Val ({best_val_accuracy:.4f}) >= 0.826. Fin de l'entraînement pour cette étape LTH.\")\n",
    "            break\n",
    "    \n",
    "    return model, best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c7c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(pruning_fraction: float = 0.2, pruning_layers_to_ignore: str = None, trained_model = None, current_mask: Mask = None) : \n",
    "    \"\"\"\n",
    "    A one iteration of pruning : returns the new updated mask after pruning.\n",
    "\n",
    "    trained_model : the original fully trained model.\n",
    "    pruning_fraction = The fraction of additional weights to prune from the network.\n",
    "    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\n",
    "    \"\"\"\n",
    "    current_mask = Mask.ones_like(trained_model).numpy() if current_mask is None else current_mask.numpy()\n",
    "\n",
    "    # Determine the number of weights that need to be pruned.\n",
    "    number_of_remaining_weights = np.sum([np.sum(v) for v in current_mask.values()])\n",
    "    number_of_weights_to_prune = np.ceil(pruning_fraction * number_of_remaining_weights).astype(int)\n",
    "\n",
    "    # Determine which layers can be pruned.\n",
    "    prunable_tensors = set(trained_model.prunable_layer_names)\n",
    "    if pruning_layers_to_ignore:\n",
    "        prunable_tensors -= set(pruning_layers_to_ignore.split(','))\n",
    "    \n",
    "    # Get the model weights.\n",
    "    weights = {k: v.clone().cpu().detach().numpy()\n",
    "                for k, v in trained_model.state_dict().items()\n",
    "                if k in prunable_tensors}\n",
    "\n",
    "    # Create a vector of all the unpruned weights in the model.\n",
    "    weight_vector = np.concatenate([v[current_mask[k] == 1] for k, v in weights.items()])\n",
    "    threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "\n",
    "    new_mask = Mask({k: np.where(np.abs(v) > threshold, current_mask[k], np.zeros_like(v))\n",
    "                        for k, v in weights.items()})\n",
    "    for k in current_mask:\n",
    "        if k not in new_mask: # if this weight was already pruned add it to the new mask\n",
    "            new_mask[k] = current_mask[k]\n",
    "\n",
    "    return new_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedModel(nn.Module): # Remplacer Model par ResNet1D \n",
    "    @staticmethod\n",
    "    def to_mask_name(name):\n",
    "        return 'mask_' + name.replace('.', '___')\n",
    "\n",
    "    def __init__(self, model: ResNet1D, mask: Mask):\n",
    "        if isinstance(model, PrunedModel): raise ValueError('Cannot nest pruned models.')\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for k in self.model.prunable_layer_names:\n",
    "            if k not in mask: raise ValueError('Missing mask value {}.'.format(k))\n",
    "            if not np.array_equal(mask[k].shape, np.array(self.model.state_dict()[k].shape)):\n",
    "                raise ValueError('Incorrect mask shape {} for tensor {}.'.format(mask[k].shape, k))\n",
    "\n",
    "        for k in mask:\n",
    "            if k not in self.model.prunable_layer_names:\n",
    "                raise ValueError('Key {} found in mask but is not a valid model tensor.'.format(k))\n",
    "\n",
    "        # for k, v in mask.items(): self.register_buffer(PrunedModel.to_mask_name(k), v.float())\n",
    "        # self._apply_mask()\n",
    "        device = next(model.parameters()).device \n",
    "\n",
    "        for k, v in mask.items(): \n",
    "            # On envoie le masque sur le même device que le modèle AVANT de l'enregistrer\n",
    "            self.register_buffer(PrunedModel.to_mask_name(k), v.float().to(device))\n",
    "            \n",
    "        self._apply_mask()\n",
    "\n",
    "    def _apply_mask(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if hasattr(self, PrunedModel.to_mask_name(name)):\n",
    "                param.data *= getattr(self, PrunedModel.to_mask_name(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._apply_mask()\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    @property\n",
    "    def prunable_layer_names(self):\n",
    "        return self.model.prunable_layer_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3267fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_priors(train_loader, device, num_classes=4, smooth=500):\n",
    "    \"\"\"Calcule la distribution des classes en gérant les labels séquentiels.\"\"\"\n",
    "    counts = torch.zeros(num_classes)\n",
    "    device = next(iter(train_loader))[1].device # Détection automatique du device\n",
    "    \n",
    "    for _, labels in train_loader:\n",
    "        # Si labels est (Batch, Séquence), on prend le premier label de chaque séquence\n",
    "        if labels.dim() > 1:\n",
    "            # labels[:, 0] récupère le premier label pour chaque échantillon du batch\n",
    "            labels = labels[:, 0]\n",
    "            \n",
    "        for l in labels:\n",
    "            counts[l.long().item()] += 1\n",
    "    \n",
    "    total = counts.sum() + num_classes\n",
    "    prior = (counts + smooth) / total\n",
    "    return prior.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "                  \"pruning_percentage\" : 30, # denoted as p in Frankle & Carbin\n",
    "                  \"nb_of_steps\" : 10, # denoted as n in Frankle & Carbin\n",
    "                  \"pruning_layers_to_ignore\" : None,\n",
    "                  \"weight_rewinding\": True\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterative_pruning(pruning_params, network, train_loader, val_loader, loss_func, resume=False) : \n",
    "    \n",
    "    prior = calculate_priors(train_loader,device,4,500).to(device)\n",
    "        \n",
    "\n",
    "    checkpoint_file = f\"iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}_checkpoint.pth\"\n",
    "    \n",
    "    # Randomly initialize the given DL network D. (quelle initialisation ? Hannun et al. -> \"He normal\")\n",
    "    pruning_percentage = pruning_params[\"pruning_percentage\"]\n",
    "    n = pruning_params['nb_of_steps']\n",
    "    current_mask = Mask.ones_like(network)\n",
    "    current_mask_np = current_mask.numpy()\n",
    "    initial_weights_number = np.sum([np.sum(v) for v in current_mask_np.values()]) # eta \n",
    "    \n",
    "    initial_untrained_model = copy.deepcopy(network)\n",
    "    \n",
    "    remaining_weights_number = initial_weights_number\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    # Listes pour l'historique\n",
    "    history_theta = []\n",
    "    history_acc = []\n",
    "    history_sparsity = []\n",
    "\n",
    "    ##########\n",
    "    if resume and os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file)\n",
    "        step = checkpoint['step']\n",
    "        pruning_percentage = checkpoint['pruning_percentage']\n",
    "        current_mask = checkpoint['current_mask']\n",
    "        history_theta = checkpoint['history_theta']\n",
    "        history_acc = checkpoint['history_acc']\n",
    "        history_sparsity = checkpoint['history_sparsity']\n",
    "        remaining_weights_number = sum(v.sum().item() for v in current_mask.values())\n",
    "        # On recharge les poids initiaux pour garantir le \"Winning Ticket\" \n",
    "        initial_untrained_model.load_state_dict(checkpoint['initial_weights'])\n",
    "        # On repart du modèle tel qu'il était avant le crash\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=current_mask).to(device)\n",
    "        print(f\"REPRISE à l'étape {step}\")   \n",
    "\n",
    "    else:\n",
    "        D = copy.deepcopy(network)\n",
    "    ##############\n",
    "    step_pruning_percentage = (pruning_percentage)**(1/n)\n",
    "    pruning_fraction = step_pruning_percentage/100\n",
    "\n",
    "    for i in range(n) : \n",
    "        \n",
    "        print(f\"\\n{'='*30} STEP {step} {'='*30}\")\n",
    "        print(f\"remaining_weights_number = {remaining_weights_number:.2e}\")\n",
    "        print(\"current reduction factor = \", np.round(initial_weights_number/remaining_weights_number, 2))\n",
    "        print(\"current pruning percentage = \", np.round(step*step_pruning_percentage,2), ' % ', ' over ', pruning_percentage, ' %')\n",
    "        print(\"pruning fraction = \", np.round(pruning_fraction,2))\n",
    "        print(\"model sparcity (percentage of weight that has been pruned): \", current_mask.sparsity )  \n",
    "        \n",
    "        # Train the DL network with the given data x.\n",
    "        D,final_accuracy = train_model(D, train_loader, val_loader, loss_func, name = \"lth_ecg\", prior = prior, epochs=20)\n",
    "\n",
    "        # 3. Archivage\n",
    "        history_theta.append(initial_weights_number/remaining_weights_number)\n",
    "        #history_acc.append(final_accuracy)\n",
    "        history_sparsity.append(current_mask.sparsity)\n",
    "\n",
    "        if isinstance(D, PrunedModel):\n",
    "            model_to_prune = D.model\n",
    "        else:\n",
    "            model_to_prune = D\n",
    "\n",
    "        # Prune p_init% of weights which are of least magnitude\n",
    "    \n",
    "        new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], model_to_prune, current_mask)\n",
    "\n",
    "        D_sparse = PrunedModel(model_to_prune, mask = new_mask)\n",
    "\n",
    "        D_sparse.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Sparse Model [step : {step}]')\n",
    "        accuracy_metric.reset() # Reset pour calculer uniquement la val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(ecgs) # Sortie probable: (Batch, Classes, Time)\n",
    "\n",
    "                # Update validation metrics\n",
    "                preds = torch.argmax(logits, dim=1)  # (B, S)\n",
    "                accuracy_metric.update(preds, targets)\n",
    "                current_val_accuracy = accuracy_metric.compute.item()\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(\n",
    "                    val_acc=f\"{current_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        history_acc.append(current_val_accuracy)\n",
    "\n",
    "\n",
    "        # Plots and push of checkpoints on github every 5 steps \n",
    "        if step % 5 == 0:\n",
    "            path = f'plots/iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}/'\n",
    "            plot_lth_progress(history_theta, history_acc, path)\n",
    "            plot_layerwise_remaining_params(D_sparse, current_mask, path)\n",
    "            # push_checkpoint_to_git(\n",
    "            #         filepaths=[checkpoint_file,\n",
    "            #         f\"plots/iterative_pruning_weight_rewinding_{str(pruning_params['weight_rewinding'])}_performance_plot.png\",\n",
    "            #         f\"plots/layerwise_remaining_params_plot_weight_rewinding_{str(pruning_params['weight_rewinding'])}.png\"],\n",
    "            #         branch_name=\"felix-v3\",  # Mets le nom de ta branche ici\n",
    "            #         commit_msg=f\"Auto save checkpoints and plots step {step-1}\"\n",
    "            #     )\n",
    "\n",
    "        # reset unpruned weights to their initial random values and D = D_sparse\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=new_mask).to(device)\n",
    "        \n",
    "        remaining_weights_number = sum(v.sum().item() for v in new_mask.values())\n",
    "       \n",
    "        current_mask = new_mask\n",
    "       \n",
    "        step+=1\n",
    "\n",
    "        # --- SAUVEGARDE DE SÉCURITÉ ---\n",
    "        checkpoint_state = {\n",
    "            'step': step,\n",
    "            'pruning_percentage': pruning_percentage,\n",
    "            'current_mask': current_mask,\n",
    "            'initial_weights': initial_untrained_model.state_dict(),\n",
    "            'reduction_factor': initial_weights_number / remaining_weights_number,\n",
    "            'history_theta': history_theta,\n",
    "            'history_acc': history_acc,\n",
    "            'history_sparsity': history_sparsity,\n",
    "            'layerwise_sparcity': current_mask.layerwise_sparsity(),\n",
    "            'layerwise_remaining_params': current_mask.layerwise_remaining_params()\n",
    "        }\n",
    "        save_checkpoint(checkpoint_state, filename=checkpoint_file)\n",
    "\n",
    "        print(\"=\"*60, \"\\n\")\n",
    "        \n",
    "\n",
    "    # Plot final à la fin de l'expérience\n",
    "    plot_lth_progress(history_theta, history_acc)\n",
    "\n",
    "    return current_mask, history_theta, history_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9c9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:01<00:00, 4323.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 4858.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN :  7.4661856  STD :  236.10312\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "MEAN :  8.029898  STD :  242.35907\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "Tri du dataset par longueur pour minimiser le padding...\n",
      "Tri du dataset par longueur pour minimiser le padding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load_dataset(\"train.json\",256)\n",
    "train_ecgs, train_labels = train\n",
    "# reduciton of size to improve training time\n",
    "# train_ecgs, train_labels = train_ecgs[:1000], train_labels[:1000]\n",
    "print(\"Loading dev set...\")\n",
    "val_ecgs,val_labels = load_dataset(\"dev.json\",256)\n",
    "# reduciton of size to improve training time\n",
    "# val_ecgs, val_labels = val_ecgs[:100], val_labels[:100]\n",
    "\n",
    "train_dataset = ECGDataset(train_ecgs, train_labels)\n",
    "val_dataset = ECGDataset(val_ecgs, val_labels)\n",
    "\n",
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f77e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_kaiming(m):\n",
    "    \"\"\"\n",
    "    Initialisation de Kaiming He (He Normal) pour les réseaux avec ReLU.\n",
    "    À utiliser avec model.apply(weights_init_kaiming).\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    # Pour les couches de Convolution (Conv1d) et Linéaires (Linear)\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        # kaiming_normal_ est la version \"He Normal\"\n",
    "        # mode='fan_out' est recommandé pour les ResNets (préserve la variance en passe avant)\n",
    "        # nonlinearity='relu' est crucial car ResNet utilise ReLU\n",
    "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        # Si la couche a un biais, on l'initialise à 0\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21393819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Initialisation Kaiming He appliquée.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=4, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose = False\n",
    "    ).to(device)\n",
    "\n",
    "# On applique l'initialisation explicite au réseau\n",
    "model.apply(weights_init_kaiming)\n",
    "print(\"Initialisation Kaiming He appliquée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9a3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 32, 100]             544\n",
      "   MyConv1dPadSame-2              [-1, 32, 100]               0\n",
      "       BatchNorm1d-3              [-1, 32, 100]              64\n",
      "              ReLU-4              [-1, 32, 100]               0\n",
      "            Conv1d-5              [-1, 32, 100]          16,416\n",
      "   MyConv1dPadSame-6              [-1, 32, 100]               0\n",
      "       BatchNorm1d-7              [-1, 32, 100]              64\n",
      "              ReLU-8              [-1, 32, 100]               0\n",
      "           Dropout-9              [-1, 32, 100]               0\n",
      "           Conv1d-10              [-1, 32, 100]          16,416\n",
      "  MyConv1dPadSame-11              [-1, 32, 100]               0\n",
      "       BasicBlock-12              [-1, 32, 100]               0\n",
      "      BatchNorm1d-13              [-1, 32, 100]              64\n",
      "             ReLU-14              [-1, 32, 100]               0\n",
      "           Conv1d-15               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-16               [-1, 32, 50]               0\n",
      "      BatchNorm1d-17               [-1, 32, 50]              64\n",
      "             ReLU-18               [-1, 32, 50]               0\n",
      "          Dropout-19               [-1, 32, 50]               0\n",
      "           Conv1d-20               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-21               [-1, 32, 50]               0\n",
      "        MaxPool1d-22               [-1, 32, 50]               0\n",
      "MyMaxPool1dPadSame-23               [-1, 32, 50]               0\n",
      "       BasicBlock-24               [-1, 32, 50]               0\n",
      "      BatchNorm1d-25               [-1, 32, 50]              64\n",
      "             ReLU-26               [-1, 32, 50]               0\n",
      "           Conv1d-27               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-28               [-1, 32, 50]               0\n",
      "      BatchNorm1d-29               [-1, 32, 50]              64\n",
      "             ReLU-30               [-1, 32, 50]               0\n",
      "          Dropout-31               [-1, 32, 50]               0\n",
      "           Conv1d-32               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-33               [-1, 32, 50]               0\n",
      "       BasicBlock-34               [-1, 32, 50]               0\n",
      "      BatchNorm1d-35               [-1, 32, 50]              64\n",
      "             ReLU-36               [-1, 32, 50]               0\n",
      "           Conv1d-37               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-38               [-1, 32, 25]               0\n",
      "      BatchNorm1d-39               [-1, 32, 25]              64\n",
      "             ReLU-40               [-1, 32, 25]               0\n",
      "          Dropout-41               [-1, 32, 25]               0\n",
      "           Conv1d-42               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-43               [-1, 32, 25]               0\n",
      "        MaxPool1d-44               [-1, 32, 25]               0\n",
      "MyMaxPool1dPadSame-45               [-1, 32, 25]               0\n",
      "       BasicBlock-46               [-1, 32, 25]               0\n",
      "      BatchNorm1d-47               [-1, 32, 25]              64\n",
      "             ReLU-48               [-1, 32, 25]               0\n",
      "           Conv1d-49               [-1, 64, 25]          32,832\n",
      "  MyConv1dPadSame-50               [-1, 64, 25]               0\n",
      "      BatchNorm1d-51               [-1, 64, 25]             128\n",
      "             ReLU-52               [-1, 64, 25]               0\n",
      "          Dropout-53               [-1, 64, 25]               0\n",
      "           Conv1d-54               [-1, 64, 25]          65,600\n",
      "  MyConv1dPadSame-55               [-1, 64, 25]               0\n",
      "       BasicBlock-56               [-1, 64, 25]               0\n",
      "      BatchNorm1d-57               [-1, 64, 25]             128\n",
      "             ReLU-58               [-1, 64, 25]               0\n",
      "           Conv1d-59               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-60               [-1, 64, 13]               0\n",
      "      BatchNorm1d-61               [-1, 64, 13]             128\n",
      "             ReLU-62               [-1, 64, 13]               0\n",
      "          Dropout-63               [-1, 64, 13]               0\n",
      "           Conv1d-64               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-65               [-1, 64, 13]               0\n",
      "        MaxPool1d-66               [-1, 64, 13]               0\n",
      "MyMaxPool1dPadSame-67               [-1, 64, 13]               0\n",
      "       BasicBlock-68               [-1, 64, 13]               0\n",
      "      BatchNorm1d-69               [-1, 64, 13]             128\n",
      "             ReLU-70               [-1, 64, 13]               0\n",
      "           Conv1d-71               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-72               [-1, 64, 13]               0\n",
      "      BatchNorm1d-73               [-1, 64, 13]             128\n",
      "             ReLU-74               [-1, 64, 13]               0\n",
      "          Dropout-75               [-1, 64, 13]               0\n",
      "           Conv1d-76               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-77               [-1, 64, 13]               0\n",
      "       BasicBlock-78               [-1, 64, 13]               0\n",
      "      BatchNorm1d-79               [-1, 64, 13]             128\n",
      "             ReLU-80               [-1, 64, 13]               0\n",
      "           Conv1d-81                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-82                [-1, 64, 7]               0\n",
      "      BatchNorm1d-83                [-1, 64, 7]             128\n",
      "             ReLU-84                [-1, 64, 7]               0\n",
      "          Dropout-85                [-1, 64, 7]               0\n",
      "           Conv1d-86                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-87                [-1, 64, 7]               0\n",
      "        MaxPool1d-88                [-1, 64, 7]               0\n",
      "MyMaxPool1dPadSame-89                [-1, 64, 7]               0\n",
      "       BasicBlock-90                [-1, 64, 7]               0\n",
      "      BatchNorm1d-91                [-1, 64, 7]             128\n",
      "             ReLU-92                [-1, 64, 7]               0\n",
      "           Conv1d-93               [-1, 128, 7]         131,200\n",
      "  MyConv1dPadSame-94               [-1, 128, 7]               0\n",
      "      BatchNorm1d-95               [-1, 128, 7]             256\n",
      "             ReLU-96               [-1, 128, 7]               0\n",
      "          Dropout-97               [-1, 128, 7]               0\n",
      "           Conv1d-98               [-1, 128, 7]         262,272\n",
      "  MyConv1dPadSame-99               [-1, 128, 7]               0\n",
      "      BasicBlock-100               [-1, 128, 7]               0\n",
      "     BatchNorm1d-101               [-1, 128, 7]             256\n",
      "            ReLU-102               [-1, 128, 7]               0\n",
      "          Conv1d-103               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-104               [-1, 128, 4]               0\n",
      "     BatchNorm1d-105               [-1, 128, 4]             256\n",
      "            ReLU-106               [-1, 128, 4]               0\n",
      "         Dropout-107               [-1, 128, 4]               0\n",
      "          Conv1d-108               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-109               [-1, 128, 4]               0\n",
      "       MaxPool1d-110               [-1, 128, 4]               0\n",
      "MyMaxPool1dPadSame-111               [-1, 128, 4]               0\n",
      "      BasicBlock-112               [-1, 128, 4]               0\n",
      "     BatchNorm1d-113               [-1, 128, 4]             256\n",
      "            ReLU-114               [-1, 128, 4]               0\n",
      "          Conv1d-115               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-116               [-1, 128, 4]               0\n",
      "     BatchNorm1d-117               [-1, 128, 4]             256\n",
      "            ReLU-118               [-1, 128, 4]               0\n",
      "         Dropout-119               [-1, 128, 4]               0\n",
      "          Conv1d-120               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-121               [-1, 128, 4]               0\n",
      "      BasicBlock-122               [-1, 128, 4]               0\n",
      "     BatchNorm1d-123               [-1, 128, 4]             256\n",
      "            ReLU-124               [-1, 128, 4]               0\n",
      "          Conv1d-125               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-126               [-1, 128, 2]               0\n",
      "     BatchNorm1d-127               [-1, 128, 2]             256\n",
      "            ReLU-128               [-1, 128, 2]               0\n",
      "         Dropout-129               [-1, 128, 2]               0\n",
      "          Conv1d-130               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-131               [-1, 128, 2]               0\n",
      "       MaxPool1d-132               [-1, 128, 2]               0\n",
      "MyMaxPool1dPadSame-133               [-1, 128, 2]               0\n",
      "      BasicBlock-134               [-1, 128, 2]               0\n",
      "     BatchNorm1d-135               [-1, 128, 2]             256\n",
      "            ReLU-136               [-1, 128, 2]               0\n",
      "          Conv1d-137               [-1, 256, 2]         524,544\n",
      " MyConv1dPadSame-138               [-1, 256, 2]               0\n",
      "     BatchNorm1d-139               [-1, 256, 2]             512\n",
      "            ReLU-140               [-1, 256, 2]               0\n",
      "         Dropout-141               [-1, 256, 2]               0\n",
      "          Conv1d-142               [-1, 256, 2]       1,048,832\n",
      " MyConv1dPadSame-143               [-1, 256, 2]               0\n",
      "      BasicBlock-144               [-1, 256, 2]               0\n",
      "     BatchNorm1d-145               [-1, 256, 2]             512\n",
      "            ReLU-146               [-1, 256, 2]               0\n",
      "          Conv1d-147               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-148               [-1, 256, 1]               0\n",
      "     BatchNorm1d-149               [-1, 256, 1]             512\n",
      "            ReLU-150               [-1, 256, 1]               0\n",
      "         Dropout-151               [-1, 256, 1]               0\n",
      "          Conv1d-152               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-153               [-1, 256, 1]               0\n",
      "       MaxPool1d-154               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-155               [-1, 256, 1]               0\n",
      "      BasicBlock-156               [-1, 256, 1]               0\n",
      "     BatchNorm1d-157               [-1, 256, 1]             512\n",
      "            ReLU-158               [-1, 256, 1]               0\n",
      "          Conv1d-159               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-160               [-1, 256, 1]               0\n",
      "     BatchNorm1d-161               [-1, 256, 1]             512\n",
      "            ReLU-162               [-1, 256, 1]               0\n",
      "         Dropout-163               [-1, 256, 1]               0\n",
      "          Conv1d-164               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-165               [-1, 256, 1]               0\n",
      "      BasicBlock-166               [-1, 256, 1]               0\n",
      "     BatchNorm1d-167               [-1, 256, 1]             512\n",
      "            ReLU-168               [-1, 256, 1]               0\n",
      "          Conv1d-169               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-170               [-1, 256, 1]               0\n",
      "     BatchNorm1d-171               [-1, 256, 1]             512\n",
      "            ReLU-172               [-1, 256, 1]               0\n",
      "         Dropout-173               [-1, 256, 1]               0\n",
      "          Conv1d-174               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-175               [-1, 256, 1]               0\n",
      "       MaxPool1d-176               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-177               [-1, 256, 1]               0\n",
      "      BasicBlock-178               [-1, 256, 1]               0\n",
      "     BatchNorm1d-179               [-1, 256, 1]             512\n",
      "            ReLU-180               [-1, 256, 1]               0\n",
      "          Conv1d-181                 [-1, 4, 1]           1,028\n",
      "================================================================\n",
      "Total params: 10,466,148\n",
      "Trainable params: 10,466,148\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.27\n",
      "Params size (MB): 39.93\n",
      "Estimated Total Size (MB): 41.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,100), device=device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434471d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== STEP 0 ==============================\n",
      "remaining_weights_number = 1.05e+07\n",
      "current reduction factor =  1.0  over 120\n",
      "pruning percentage =  30  %\n",
      "pruning fraction =  0.3\n",
      "model sparcity (percentage of weight that has been pruned):  tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]: 100%|███████████████████████| 240/240 [00:50<00:00,  4.71it/s, curr_train_f1=0.3444, curr_train_loss=1.15268212, lr=0.00100000]\n",
      "Validation Epoch [1/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:01<00:00, 14.42it/s, f1_val=0.3878, val_loss=1.19400441]\n",
      "Training Epoch [2/20]: 100%|███████████████████████| 240/240 [00:53<00:00,  4.50it/s, curr_train_f1=0.4976, curr_train_loss=0.84971344, lr=0.00100000]\n",
      "Validation Epoch [2/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:01<00:00, 14.18it/s, f1_val=0.5665, val_loss=0.82721945]\n",
      "Training Epoch [3/20]: 100%|███████████████████████| 240/240 [00:53<00:00,  4.48it/s, curr_train_f1=0.6193, curr_train_loss=0.74308664, lr=0.00100000]\n",
      "Validation Epoch [3/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:01<00:00, 14.29it/s, f1_val=0.6550, val_loss=0.76247418]\n",
      "Training Epoch [4/20]:  69%|███████████████▊       | 165/240 [00:36<00:15,  4.75it/s, curr_train_f1=0.6908, curr_train_loss=0.72719438, lr=0.00100000]"
     ]
    }
   ],
   "source": [
    "final_mask, history_theta, history_f1 = run_lth_ecg(pruning_params, model, train_loader, val_loader, loss_func) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajouter le nombre de MB que fait le modèle au fur-et-à-mesure, ça se trouve les 1 mb ne sont pas si loin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e19e8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphique mis à jour et sauvegardé dans : plots/lth_performance_plot.png\n",
      "current_mask is a mask ?  True\n",
      "prunable_tensors : \n",
      " {'basicblock_list.10.conv1.conv.weight', 'basicblock_list.12.conv2.conv.weight', 'basicblock_list.14.conv1.conv.weight', 'basicblock_list.6.conv1.conv.weight', 'basicblock_list.0.conv2.conv.weight', 'basicblock_list.2.conv2.conv.weight', 'basicblock_list.1.conv2.conv.weight', 'basicblock_list.3.conv2.conv.weight', 'basicblock_list.5.conv2.conv.weight', 'basicblock_list.7.conv1.conv.weight', 'basicblock_list.2.conv1.conv.weight', 'basicblock_list.5.conv1.conv.weight', 'basicblock_list.10.conv2.conv.weight', 'basicblock_list.8.conv2.conv.weight', 'basicblock_list.11.conv1.conv.weight', 'basicblock_list.15.conv1.conv.weight', 'basicblock_list.12.conv1.conv.weight', 'basicblock_list.15.conv2.conv.weight', 'basicblock_list.14.conv2.conv.weight', 'basicblock_list.7.conv2.conv.weight', 'basicblock_list.4.conv1.conv.weight', 'first_block_conv.conv.weight', 'basicblock_list.1.conv1.conv.weight', 'basicblock_list.11.conv2.conv.weight', 'basicblock_list.9.conv2.conv.weight', 'basicblock_list.6.conv2.conv.weight', 'basicblock_list.13.conv2.conv.weight', 'basicblock_list.4.conv2.conv.weight', 'dense.weight', 'basicblock_list.13.conv1.conv.weight', 'basicblock_list.0.conv1.conv.weight', 'basicblock_list.8.conv1.conv.weight', 'basicblock_list.9.conv1.conv.weight', 'basicblock_list.3.conv1.conv.weight'}\n",
      "new_mask is a mask ?  True\n",
      "--> Sauvegarde du checkpoint : lth_ecg_checkpoint.pth\n",
      "============================================================ \n",
      "\n",
      "\n",
      "============================== STEP 21 ==============================\n",
      "remaining_weights_number = 2.05e+06\n",
      "current reduction factor =  5.09\n",
      "pruning percentage =  1.52\n",
      "pruning fraction =  0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]: 100%|███████████████████████| 240/240 [01:03<00:00,  3.78it/s, curr_train_f1=0.7714, curr_train_loss=0.50106471, lr=0.00100000]\n",
      "Validation Epoch [1/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.85it/s, f1_val=0.8080, val_loss=0.39414138]\n",
      "Training Epoch [2/20]: 100%|███████████████████████| 240/240 [01:03<00:00,  3.78it/s, curr_train_f1=0.8181, curr_train_loss=0.43517453, lr=0.00100000]\n",
      "Validation Epoch [2/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.98it/s, f1_val=0.7869, val_loss=0.36292392]\n",
      "Training Epoch [3/20]: 100%|███████████████████████| 240/240 [01:03<00:00,  3.78it/s, curr_train_f1=0.8307, curr_train_loss=0.39576271, lr=0.00100000]\n",
      "Validation Epoch [3/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.93it/s, f1_val=0.8002, val_loss=0.32510099]\n",
      "Training Epoch [4/20]: 100%|███████████████████████| 240/240 [01:03<00:00,  3.79it/s, curr_train_f1=0.8339, curr_train_loss=0.42309269, lr=0.00100000]\n",
      "Validation Epoch [4/20]: 100%|████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.84it/s, f1_val=0.8533, val_loss=0.46618035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] F1 Val (0.8533) >= 0.826. Fin de l'entraînement pour cette étape LTH.\n",
      "current_mask is a mask ?  True\n",
      "prunable_tensors : \n",
      " {'basicblock_list.10.conv1.conv.weight', 'basicblock_list.12.conv2.conv.weight', 'basicblock_list.14.conv1.conv.weight', 'basicblock_list.6.conv1.conv.weight', 'basicblock_list.0.conv2.conv.weight', 'basicblock_list.2.conv2.conv.weight', 'basicblock_list.1.conv2.conv.weight', 'basicblock_list.3.conv2.conv.weight', 'basicblock_list.5.conv2.conv.weight', 'basicblock_list.7.conv1.conv.weight', 'basicblock_list.2.conv1.conv.weight', 'basicblock_list.5.conv1.conv.weight', 'basicblock_list.10.conv2.conv.weight', 'basicblock_list.8.conv2.conv.weight', 'basicblock_list.11.conv1.conv.weight', 'basicblock_list.15.conv1.conv.weight', 'basicblock_list.12.conv1.conv.weight', 'basicblock_list.15.conv2.conv.weight', 'basicblock_list.14.conv2.conv.weight', 'basicblock_list.7.conv2.conv.weight', 'basicblock_list.4.conv1.conv.weight', 'first_block_conv.conv.weight', 'basicblock_list.1.conv1.conv.weight', 'basicblock_list.11.conv2.conv.weight', 'basicblock_list.9.conv2.conv.weight', 'basicblock_list.6.conv2.conv.weight', 'basicblock_list.13.conv2.conv.weight', 'basicblock_list.4.conv2.conv.weight', 'dense.weight', 'basicblock_list.13.conv1.conv.weight', 'basicblock_list.0.conv1.conv.weight', 'basicblock_list.8.conv1.conv.weight', 'basicblock_list.9.conv1.conv.weight', 'basicblock_list.3.conv1.conv.weight'}\n",
      "new_mask is a mask ?  True\n",
      "--> Sauvegarde du checkpoint : lth_ecg_checkpoint.pth\n",
      "============================================================ \n",
      "\n",
      "\n",
      "============================== STEP 22 ==============================\n",
      "remaining_weights_number = 2.02e+06\n",
      "current reduction factor =  5.17\n",
      "pruning percentage =  1.46\n",
      "pruning fraction =  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]:  38%|█████████               | 90/240 [00:23<00:39,  3.82it/s, curr_train_f1=0.7557, curr_train_loss=0.52298746, lr=0.00100000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_mask, history_theta, history_f1 = \u001b[43mrun_lth_ecg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mrun_lth_ecg\u001b[39m\u001b[34m(pruning_params, network, train_loader, val_loader, loss_func, resume)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpruning fraction = \u001b[39m\u001b[33m\"\u001b[39m, np.round(pruning_fraction,\u001b[32m2\u001b[39m))\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Train the DL network with the given data x.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m D,final_f1 = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlth_ecg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 3. Archivage\u001b[39;00m\n\u001b[32m     65\u001b[39m history_theta.append(initial_weights_number/remaining_weights_number)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, loss_func, tb_logger, prior, epochs, name)\u001b[39m\n\u001b[32m     36\u001b[39m ecgs, labels = batch \u001b[38;5;66;03m# Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\u001b[39;00m\n\u001b[32m     37\u001b[39m ecgs, labels = ecgs.to(device), labels.to(device) \u001b[38;5;66;03m# Send the data to the device (GPU or CPU) - it has to be the same device as the model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Stage 1: Forward().\u001b[39;00m\n\u001b[32m     41\u001b[39m loss = loss_func(pred, labels) \u001b[38;5;66;03m# Compute the loss over the predictions and the ground truth.\u001b[39;00m\n\u001b[32m     42\u001b[39m loss.backward()  \u001b[38;5;66;03m# Stage 2: Backward().\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mPrunedModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_mask()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/resnet1d.py:289\u001b[39m, in \u001b[36mResNet1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mi_block: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, in_channels: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m, out_channels: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m, downsample: \u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[33m'\u001b[39m.format(i_block, net.in_channels, net.out_channels, net.downsample))\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m out = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mprint\u001b[39m(out.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Advanced-ML-Project/resnet1d.py:157\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# the second conv\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_bn:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu2(out)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_do:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/functional.py:2813\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2811\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "final_mask, history_theta, history_f1 = run_lth_ecg(pruning_params, model, train_loader, val_loader, loss_func, resume=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
