{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a973793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler, load_dataset, load_ecg\n",
    "from resnet1d import ResNet1D\n",
    "from mask import Mask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "    # Configuration du F1-Score pour 4 classes (N, A, O, ~)\n",
    "    # On utilise 'macro' pour donner autant d'importance à chaque classe\n",
    "    f1_metric = MulticlassF1Score(num_classes=4, average='macro').to(device)\n",
    "    best_f1_val = 0.0\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )\n",
    "                                                    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training stage, where we want to update the parameters.\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        f1_metric.reset()\n",
    "\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            ecgs, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            ecgs, labels = ecgs.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "\n",
    "            pred = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            # scheduler.step() # Update the learning rate.\n",
    "\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "            f1_metric.update(pred, labels) # Accumulation pour le F1 train\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      curr_train_f1=f\"{f1_metric.compute():.4f}\",\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "                                      )\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            #tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        f1_metric.reset() # Reset pour calculer uniquement la val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                pred = model(ecgs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "                f1_metric.update(pred, labels)\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(\n",
    "                    val_loss = \"{:.8f}\".format(np.mean(validation_loss)),\n",
    "                    f1_val=f\"{f1_metric.compute():.4f}\")\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                #tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "        \n",
    "        \n",
    "        # Calcul final des métriques de l'époque\n",
    "        epoch_f1_val = f1_metric.compute().item()\n",
    "        epoch_loss_val = np.mean(validation_loss)\n",
    "        \n",
    "        if epoch_f1_val > best_f1_val:\n",
    "            best_f1_val = epoch_f1_val\n",
    "\n",
    "        scheduler.step(np.mean(validation_loss))\n",
    "\n",
    "        # --- LOGIQUE D'EARLY STOPPING ---\n",
    "        # Si le F1 dépasse 82.6% (0.826), on considère que le ticket a convergé avec une tolérance de 1%\n",
    "        # par rapport au 83.6% de l'article\n",
    "        if epoch_f1_val >= 0.826:\n",
    "            print(f\"\\n[Early Stopping] F1 Val ({epoch_f1_val:.4f}) >= 0.826. Fin de l'entraînement pour cette étape LTH.\")\n",
    "            break\n",
    "    \n",
    "    return model, best_f1_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7791d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def plot_lth_progress(history_theta, history_f1):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Courbe de ton modèle\n",
    "    plt.plot(history_theta, history_f1, 'r-o', label='LTH-ECG (Ton run)')\n",
    "    \n",
    "    # Ligne de référence du papier (F1 = 0.836) \n",
    "    plt.axhline(y=0.836, color='black', linestyle='--', label='Benchmark (0.836)')\n",
    "    \n",
    "    # Zone de tolérance de 1% (0.836 * 0.99 = 0.827) [cite: 119, 158]\n",
    "    plt.axhline(y=0.827, color='gray', linestyle=':', label='Tolérance 1%')\n",
    "    \n",
    "    plt.xlabel('Parameter reduction factor (theta)') [cite: 157]\n",
    "    plt.ylabel('Test mean F1-score') [cite: 143]\n",
    "    plt.title('LTH-ECG: Évolution de la performance')\n",
    "    plt.xscale('log') # Utile pour voir les grandes compressions (1x à 142x)\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(pruning_fraction: float = 0.2, pruning_layers_to_ignore: str = None, trained_model = None, current_mask: Mask = None) : \n",
    "    \"\"\"\n",
    "    A one iteration of pruning : returns the new updated mask after pruning.\n",
    "\n",
    "    trained_model : the original fully trained model.\n",
    "    pruning_fraction = The fraction of additional weights to prune from the network.\n",
    "    layers_to_ignore = A comma-separated list of addititonal tensors that should not be pruned.\n",
    "    \"\"\"\n",
    "    current_mask = Mask.ones_like(trained_model).numpy() if current_mask is None else current_mask.numpy()\n",
    "\n",
    "    # Determine the number of weights that need to be pruned.\n",
    "    number_of_remaining_weights = np.sum([np.sum(v) for v in current_mask.values()])\n",
    "    number_of_weights_to_prune = np.ceil(pruning_fraction * number_of_remaining_weights).astype(int)\n",
    "\n",
    "    # Determine which layers can be pruned.\n",
    "    prunable_tensors = set(trained_model.prunable_layer_names)\n",
    "    if pruning_layers_to_ignore:\n",
    "        prunable_tensors -= set(pruning_layers_to_ignore.split(','))\n",
    "    print(\"prunable_tensors : \\n\", prunable_tensors)\n",
    "    # Get the model weights.\n",
    "    weights = {k: v.clone().cpu().detach().numpy()\n",
    "                for k, v in trained_model.state_dict().items()\n",
    "                if k in prunable_tensors}\n",
    "\n",
    "    # Create a vector of all the unpruned weights in the model.\n",
    "    weight_vector = np.concatenate([v[current_mask[k] == 1] for k, v in weights.items()])\n",
    "    threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "\n",
    "    new_mask = Mask({k: np.where(np.abs(v) > threshold, current_mask[k], np.zeros_like(v))\n",
    "                        for k, v in weights.items()})\n",
    "    for k in current_mask:\n",
    "        if k not in new_mask: # if this weight was already pruned add it to the new mask\n",
    "            new_mask[k] = current_mask[k]\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedModel(nn.Module): # Remplacer Model par ResNet1D \n",
    "    @staticmethod\n",
    "    def to_mask_name(name):\n",
    "        return 'mask_' + name.replace('.', '___')\n",
    "\n",
    "    def __init__(self, model: ResNet1D, mask: Mask):\n",
    "        if isinstance(model, PrunedModel): raise ValueError('Cannot nest pruned models.')\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for k in self.model.prunable_layer_names:\n",
    "            if k not in mask: raise ValueError('Missing mask value {}.'.format(k))\n",
    "            if not np.array_equal(mask[k].shape, np.array(self.model.state_dict()[k].shape)):\n",
    "                raise ValueError('Incorrect mask shape {} for tensor {}.'.format(mask[k].shape, k))\n",
    "\n",
    "        for k in mask:\n",
    "            if k not in self.model.prunable_layer_names:\n",
    "                raise ValueError('Key {} found in mask but is not a valid model tensor.'.format(k))\n",
    "\n",
    "        # for k, v in mask.items(): self.register_buffer(PrunedModel.to_mask_name(k), v.float())\n",
    "        # self._apply_mask()\n",
    "        device = next(model.parameters()).device \n",
    "\n",
    "        for k, v in mask.items(): \n",
    "            # On envoie le masque sur le même device que le modèle AVANT de l'enregistrer\n",
    "            self.register_buffer(PrunedModel.to_mask_name(k), v.float().to(device))\n",
    "            \n",
    "        self._apply_mask()\n",
    "\n",
    "    def _apply_mask(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if hasattr(self, PrunedModel.to_mask_name(name)):\n",
    "                param.data *= getattr(self, PrunedModel.to_mask_name(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._apply_mask()\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    @property\n",
    "    def prunable_layer_names(self):\n",
    "        return self.model.prunable_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def output_layer_names(self):\n",
    "    #     return self.model.output_layer_names\n",
    "\n",
    "    # @property\n",
    "    # def loss_criterion(self):\n",
    "    #     return self.model.loss_criterion\n",
    "\n",
    "    # def save(self, save_location, save_step):\n",
    "    #     self.model.save(save_location, save_step)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def default_hparams(): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def is_valid_model_name(model_name): raise NotImplementedError()\n",
    "    # @staticmethod\n",
    "    # def get_model_from_name(model_name, outputs, initializer): raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"lth_checkpoint.pth\"):\n",
    "    \"\"\"Sauvegarde l'état complet de l'expérience.\"\"\"\n",
    "    print(f\"--> Sauvegarde du checkpoint : {filename}\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename=\"lth_checkpoint.pth\"):\n",
    "    \"\"\"Charge l'état pour reprendre l'expérience.\"\"\"\n",
    "    print(f\"--> Chargement du checkpoint : {filename}\")\n",
    "    return torch.load(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[30**(1/1.1**n) for n in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "                  \"p_init\" : 30,\n",
    "                  \"target_reduction_factor\" : 120, \n",
    "                  \"alpha\" : 1.1,\n",
    "                  \"pruning_layers_to_ignore\" : None\n",
    "                  }\n",
    "\n",
    "\n",
    "def run_lth_ecg(pruning_params, network, train_loader, val_loader, loss_func, resume=False) : \n",
    "    checkpoint_file = \"lth_ecg_checkpoint.pth\"\n",
    "    # Randomly initialize the given DL network D. (quelle initialisation ? Hannun et al. -> \"He normal\")\n",
    "    pruning_percentage = pruning_params[\"p_init\"]\n",
    "    \n",
    "    current_mask = Mask.ones_like(network)\n",
    "    current_mask_np = current_mask.numpy()\n",
    "    initial_weights_number = np.sum([np.sum(v) for v in current_mask_np.values()]) # eta \n",
    "    \n",
    "    print(f\"eta = {initial_weights_number:.2e}\")\n",
    "    # current_model = network\n",
    "    initial_untrained_model = copy.deepcopy(network)\n",
    "    \n",
    "    remaining_weights_number = initial_weights_number\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    # Listes pour l'historique\n",
    "    history_theta = []\n",
    "    history_f1 = []\n",
    "\n",
    "    ##########\n",
    "    if resume and os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file)\n",
    "        step = checkpoint['step']\n",
    "        pruning_percentage = checkpoint['pruning_percentage']\n",
    "        current_mask = checkpoint['current_mask']\n",
    "        remaining_weights_number = sum(v.sum().item() for v in current_mask.values())\n",
    "        # On recharge les poids initiaux pour garantir le \"Winning Ticket\" \n",
    "        initial_untrained_model.load_state_dict(checkpoint['initial_weights'])\n",
    "        # On repart du modèle tel qu'il était avant le crash\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=current_mask).to(device)\n",
    "        print(f\"REPRISE à l'étape {step}\")   \n",
    "\n",
    "    else:\n",
    "        D = copy.deepcopy(network)\n",
    "    ##############\n",
    "\n",
    "    # D = copy.deepcopy(network) #current_network\n",
    "    \n",
    "    while (initial_weights_number/remaining_weights_number) < pruning_params[\"target_reduction_factor\"]:\n",
    "        pruning_fraction = pruning_percentage/100\n",
    "        \n",
    "        print(f\"\\n{'='*30} STEP {step} {'='*30}\")\n",
    "        print(f\"remaining_weights_number = {remaining_weights_number:.2e}\")\n",
    "        print(\"current reduction factor = \", np.round(initial_weights_number/remaining_weights_number, 2))\n",
    "        print(\"pruning percentage = \", np.round(pruning_percentage,2))\n",
    "        print(\"pruning fraction = \", np.round(pruning_fraction,2))\n",
    "        \n",
    "        # Train the DL network with the given data x.\n",
    "        D,final_f1 = train_model(D, train_loader, val_loader, loss_func, name = \"lth_ecg\", epochs=1,tb_logger=None)\n",
    "\n",
    "        # 3. Archivage\n",
    "        history_theta.append(initial_weights_number/remaining_weights_number)\n",
    "        history_f1.append(final_f1)\n",
    "\n",
    "        # 4. Affichage toutes les 5 steps\n",
    "        if step % 5 == 0:\n",
    "            plot_lth_progress(history_theta, history_f1)\n",
    "\n",
    "        if isinstance(D, PrunedModel):\n",
    "            model_to_prune = D.model\n",
    "        else:\n",
    "            model_to_prune = D\n",
    "        print(\"current_mask is a mask ? \", isinstance(current_mask, Mask))\n",
    "        # Prune p_init% of weights which are of least magnitude\n",
    "        #new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], D)\n",
    "        new_mask = prune(pruning_fraction, pruning_params[\"pruning_layers_to_ignore\"], model_to_prune, current_mask)\n",
    "        print(\"new_mask is a mask ? \", isinstance(new_mask, Mask))\n",
    "        # print(\"new_mask : \\n \", new_mask)\n",
    "        \n",
    "\n",
    "        # alpha = 1.1\n",
    "        pruning_percentage = pruning_percentage**(1/1.1)\n",
    "        # reset unpruned weights to their initial random values and D = D_sparse\n",
    "        D = PrunedModel(model=copy.deepcopy(initial_untrained_model), mask=new_mask).to(device)\n",
    "        \n",
    "        # remaining_weights_number = # On utilise la somme native de Python, et .item() pour extraire la valeur du tenseur\n",
    "        remaining_weights_number = sum(v.sum().item() for v in new_mask.values())\n",
    "        # print(\"just before current_mask = new_mask new_mask is a mask ? \", isinstance(new_mask, Mask))\n",
    "        # print(\"just before current_mask = new_mask current_mask is a mask ? \", isinstance(current_mask, Mask))\n",
    "        current_mask = new_mask\n",
    "        # print(\"just after current_mask = new_mask current_mask is a mask ? \", isinstance(current_mask, Mask))\n",
    "        # print(\"just after current_mask = new_mask current_mask is a dict ? \", isinstance(current_mask, dict))\n",
    "\n",
    "        # --- SAUVEGARDE DE SÉCURITÉ ---\n",
    "        checkpoint_state = {\n",
    "            'step': step,\n",
    "            'pruning_percentage': pruning_percentage,\n",
    "            'current_mask': current_mask,\n",
    "            'initial_weights': initial_untrained_model.state_dict(),\n",
    "            'reduction_factor': initial_weights_number / remaining_weights_number\n",
    "        }\n",
    "        save_checkpoint(checkpoint_state, filename=checkpoint_file)\n",
    "\n",
    "        print(\"=\"*60, \"\\n\")\n",
    "        step+=1\n",
    "\n",
    "    # Plot final à la fin de l'expérience\n",
    "    plot_lth_progress(history_theta, history_f1)\n",
    "\n",
    "    return current_mask, history_theta, history_f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load_dataset(\"train.json\",256)\n",
    "train_ecgs, train_labels = train\n",
    "# reduciton of size to improve training time\n",
    "# train_ecgs, train_labels = train_ecgs[:1000], train_labels[:1000]\n",
    "print(\"Loading dev set...\")\n",
    "val_ecgs,val_labels = load_dataset(\"dev.json\",256)\n",
    "# reduciton of size to improve training time\n",
    "# val_ecgs, val_labels = val_ecgs[:100], val_labels[:100]\n",
    "\n",
    "train_dataset = ECGDataset(train_ecgs, train_labels)\n",
    "val_dataset = ECGDataset(val_ecgs, val_labels)\n",
    "\n",
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21393819",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=4, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose = False\n",
    "    ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434471d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask, history_theta, history_f1 = run_lth_ecg(pruning_params, model, train_loader, val_loader, loss_func) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
