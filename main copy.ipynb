{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b3d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import load \n",
    "from load_data import ECGDataset, ECGCollate, SmartBatchSampler\n",
    "from resnet1d import ResNet1D\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b9cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #128 dans le papier de Hannun et al. mais 32 dans le config.json leurs test sur cinc17...\n",
    "\n",
    "writer = SummaryWriter('runs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa330672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9d4fa3a7ddbf4938\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9d4fa3a7ddbf4938\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783b6677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:01<00:00, 4935.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X) : \n",
      " 7676\n",
      "len(y) : \n",
      " 7676\n",
      "ecg_0 : \n",
      " [  72   83   93 ... -136 -133 -131]\n",
      "len(ecg_0) : 8960\n",
      "label_0 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_0) : 35\n",
      "ecg_1 : \n",
      " [-137 -167 -200 ...  -50  -51  -51]\n",
      "len(ecg_1) : 8960\n",
      "label_1 :\n",
      " ['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "len(label_1) : 35\n",
      "ecg_2 : \n",
      " [620 780 914 ... 102 115 116]\n",
      "len(ecg_2) : 8960\n",
      "label_2 :\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "len(label_2) : 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training set...\")\n",
    "train = load.load_dataset(\"train.json\")\n",
    "ecgs, labels = train\n",
    "print(f\"len(X) : \\n {len(ecgs)}\")\n",
    "print(f\"len(y) : \\n {len(labels)}\")\n",
    "for i in range(3):\n",
    "    print(f\"ecg_{i} : \\n {ecgs[i]}\")\n",
    "    print(f\"len(ecg_{i}) : {len(ecgs[i])}\")\n",
    "    print(f\"label_{i} :\\n {labels[i]}\")\n",
    "    print(f\"len(label_{i}) : {len(labels[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a71082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 5502.94it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dev set...\")\n",
    "val = load.load_dataset(\"dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b41d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN :  7.4661856  STD :  236.10312\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n",
      "MEAN :  8.029898  STD :  242.35907\n",
      "self.classes :  ['A', 'N', 'O', '~']\n",
      "self.class_to_int :  {'A': 0, 'N': 1, 'O': 2, '~': 3}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ECGDataset(*train)\n",
    "val_dataset = ECGDataset(*val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4996a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri du dataset par longueur pour minimiser le padding...\n",
      "Tri du dataset par longueur pour minimiser le padding...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instanciation du Sampler intelligent\n",
    "train_batch_sampler = SmartBatchSampler(train_dataset, 32)\n",
    "val_batch_sampler = SmartBatchSampler(val_dataset, 32)\n",
    "\n",
    "train_collate_fn = ECGCollate(\n",
    "    pad_val_x=train_dataset.pad_value_x_normalized,\n",
    "    num_classes=train_dataset.num_classes\n",
    ")\n",
    "\n",
    "val_collate_fn = ECGCollate(\n",
    "    pad_val_x=val_dataset.pad_value_x_normalized,\n",
    "    num_classes=val_dataset.num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=train_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_batch_sampler, \n",
    "    collate_fn=val_collate_fn,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ded284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-761bcfa38fa838b0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-761bcfa38fa838b0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6674055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "input shape torch.Size([2, 1, 100])\n",
      "after first conv torch.Size([2, 32, 100])\n",
      "i_block: 0, in_channels: 32, out_channels: 32, downsample: False\n",
      "torch.Size([2, 32, 100])\n",
      "i_block: 1, in_channels: 32, out_channels: 32, downsample: True\n",
      "torch.Size([2, 32, 50])\n",
      "i_block: 2, in_channels: 32, out_channels: 32, downsample: False\n",
      "torch.Size([2, 32, 50])\n",
      "i_block: 3, in_channels: 32, out_channels: 32, downsample: True\n",
      "torch.Size([2, 32, 25])\n",
      "i_block: 4, in_channels: 32, out_channels: 64, downsample: False\n",
      "torch.Size([2, 64, 25])\n",
      "i_block: 5, in_channels: 64, out_channels: 64, downsample: True\n",
      "torch.Size([2, 64, 13])\n",
      "i_block: 6, in_channels: 64, out_channels: 64, downsample: False\n",
      "torch.Size([2, 64, 13])\n",
      "i_block: 7, in_channels: 64, out_channels: 64, downsample: True\n",
      "torch.Size([2, 64, 7])\n",
      "i_block: 8, in_channels: 64, out_channels: 128, downsample: False\n",
      "torch.Size([2, 128, 7])\n",
      "i_block: 9, in_channels: 128, out_channels: 128, downsample: True\n",
      "torch.Size([2, 128, 4])\n",
      "i_block: 10, in_channels: 128, out_channels: 128, downsample: False\n",
      "torch.Size([2, 128, 4])\n",
      "i_block: 11, in_channels: 128, out_channels: 128, downsample: True\n",
      "torch.Size([2, 128, 2])\n",
      "i_block: 12, in_channels: 128, out_channels: 256, downsample: False\n",
      "torch.Size([2, 256, 2])\n",
      "i_block: 13, in_channels: 256, out_channels: 256, downsample: True\n",
      "torch.Size([2, 256, 1])\n",
      "i_block: 14, in_channels: 256, out_channels: 256, downsample: False\n",
      "torch.Size([2, 256, 1])\n",
      "i_block: 15, in_channels: 256, out_channels: 256, downsample: True\n",
      "torch.Size([2, 256, 1])\n",
      "final pooling torch.Size([2, 256])\n",
      "dense torch.Size([2, 2])\n",
      "softmax torch.Size([2, 2])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 32, 100]             544\n",
      "   MyConv1dPadSame-2              [-1, 32, 100]               0\n",
      "       BatchNorm1d-3              [-1, 32, 100]              64\n",
      "              ReLU-4              [-1, 32, 100]               0\n",
      "            Conv1d-5              [-1, 32, 100]          16,416\n",
      "   MyConv1dPadSame-6              [-1, 32, 100]               0\n",
      "       BatchNorm1d-7              [-1, 32, 100]              64\n",
      "              ReLU-8              [-1, 32, 100]               0\n",
      "           Dropout-9              [-1, 32, 100]               0\n",
      "           Conv1d-10              [-1, 32, 100]          16,416\n",
      "  MyConv1dPadSame-11              [-1, 32, 100]               0\n",
      "       BasicBlock-12              [-1, 32, 100]               0\n",
      "      BatchNorm1d-13              [-1, 32, 100]              64\n",
      "             ReLU-14              [-1, 32, 100]               0\n",
      "           Conv1d-15               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-16               [-1, 32, 50]               0\n",
      "      BatchNorm1d-17               [-1, 32, 50]              64\n",
      "             ReLU-18               [-1, 32, 50]               0\n",
      "          Dropout-19               [-1, 32, 50]               0\n",
      "           Conv1d-20               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-21               [-1, 32, 50]               0\n",
      "        MaxPool1d-22               [-1, 32, 50]               0\n",
      "MyMaxPool1dPadSame-23               [-1, 32, 50]               0\n",
      "       BasicBlock-24               [-1, 32, 50]               0\n",
      "      BatchNorm1d-25               [-1, 32, 50]              64\n",
      "             ReLU-26               [-1, 32, 50]               0\n",
      "           Conv1d-27               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-28               [-1, 32, 50]               0\n",
      "      BatchNorm1d-29               [-1, 32, 50]              64\n",
      "             ReLU-30               [-1, 32, 50]               0\n",
      "          Dropout-31               [-1, 32, 50]               0\n",
      "           Conv1d-32               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-33               [-1, 32, 50]               0\n",
      "       BasicBlock-34               [-1, 32, 50]               0\n",
      "      BatchNorm1d-35               [-1, 32, 50]              64\n",
      "             ReLU-36               [-1, 32, 50]               0\n",
      "           Conv1d-37               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-38               [-1, 32, 25]               0\n",
      "      BatchNorm1d-39               [-1, 32, 25]              64\n",
      "             ReLU-40               [-1, 32, 25]               0\n",
      "          Dropout-41               [-1, 32, 25]               0\n",
      "           Conv1d-42               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-43               [-1, 32, 25]               0\n",
      "        MaxPool1d-44               [-1, 32, 25]               0\n",
      "MyMaxPool1dPadSame-45               [-1, 32, 25]               0\n",
      "       BasicBlock-46               [-1, 32, 25]               0\n",
      "      BatchNorm1d-47               [-1, 32, 25]              64\n",
      "             ReLU-48               [-1, 32, 25]               0\n",
      "           Conv1d-49               [-1, 64, 25]          32,832\n",
      "  MyConv1dPadSame-50               [-1, 64, 25]               0\n",
      "      BatchNorm1d-51               [-1, 64, 25]             128\n",
      "             ReLU-52               [-1, 64, 25]               0\n",
      "          Dropout-53               [-1, 64, 25]               0\n",
      "           Conv1d-54               [-1, 64, 25]          65,600\n",
      "  MyConv1dPadSame-55               [-1, 64, 25]               0\n",
      "       BasicBlock-56               [-1, 64, 25]               0\n",
      "      BatchNorm1d-57               [-1, 64, 25]             128\n",
      "             ReLU-58               [-1, 64, 25]               0\n",
      "           Conv1d-59               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-60               [-1, 64, 13]               0\n",
      "      BatchNorm1d-61               [-1, 64, 13]             128\n",
      "             ReLU-62               [-1, 64, 13]               0\n",
      "          Dropout-63               [-1, 64, 13]               0\n",
      "           Conv1d-64               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-65               [-1, 64, 13]               0\n",
      "        MaxPool1d-66               [-1, 64, 13]               0\n",
      "MyMaxPool1dPadSame-67               [-1, 64, 13]               0\n",
      "       BasicBlock-68               [-1, 64, 13]               0\n",
      "      BatchNorm1d-69               [-1, 64, 13]             128\n",
      "             ReLU-70               [-1, 64, 13]               0\n",
      "           Conv1d-71               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-72               [-1, 64, 13]               0\n",
      "      BatchNorm1d-73               [-1, 64, 13]             128\n",
      "             ReLU-74               [-1, 64, 13]               0\n",
      "          Dropout-75               [-1, 64, 13]               0\n",
      "           Conv1d-76               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-77               [-1, 64, 13]               0\n",
      "       BasicBlock-78               [-1, 64, 13]               0\n",
      "      BatchNorm1d-79               [-1, 64, 13]             128\n",
      "             ReLU-80               [-1, 64, 13]               0\n",
      "           Conv1d-81                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-82                [-1, 64, 7]               0\n",
      "      BatchNorm1d-83                [-1, 64, 7]             128\n",
      "             ReLU-84                [-1, 64, 7]               0\n",
      "          Dropout-85                [-1, 64, 7]               0\n",
      "           Conv1d-86                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-87                [-1, 64, 7]               0\n",
      "        MaxPool1d-88                [-1, 64, 7]               0\n",
      "MyMaxPool1dPadSame-89                [-1, 64, 7]               0\n",
      "       BasicBlock-90                [-1, 64, 7]               0\n",
      "      BatchNorm1d-91                [-1, 64, 7]             128\n",
      "             ReLU-92                [-1, 64, 7]               0\n",
      "           Conv1d-93               [-1, 128, 7]         131,200\n",
      "  MyConv1dPadSame-94               [-1, 128, 7]               0\n",
      "      BatchNorm1d-95               [-1, 128, 7]             256\n",
      "             ReLU-96               [-1, 128, 7]               0\n",
      "          Dropout-97               [-1, 128, 7]               0\n",
      "           Conv1d-98               [-1, 128, 7]         262,272\n",
      "  MyConv1dPadSame-99               [-1, 128, 7]               0\n",
      "      BasicBlock-100               [-1, 128, 7]               0\n",
      "     BatchNorm1d-101               [-1, 128, 7]             256\n",
      "            ReLU-102               [-1, 128, 7]               0\n",
      "          Conv1d-103               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-104               [-1, 128, 4]               0\n",
      "     BatchNorm1d-105               [-1, 128, 4]             256\n",
      "            ReLU-106               [-1, 128, 4]               0\n",
      "         Dropout-107               [-1, 128, 4]               0\n",
      "          Conv1d-108               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-109               [-1, 128, 4]               0\n",
      "       MaxPool1d-110               [-1, 128, 4]               0\n",
      "MyMaxPool1dPadSame-111               [-1, 128, 4]               0\n",
      "      BasicBlock-112               [-1, 128, 4]               0\n",
      "     BatchNorm1d-113               [-1, 128, 4]             256\n",
      "            ReLU-114               [-1, 128, 4]               0\n",
      "          Conv1d-115               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-116               [-1, 128, 4]               0\n",
      "     BatchNorm1d-117               [-1, 128, 4]             256\n",
      "            ReLU-118               [-1, 128, 4]               0\n",
      "         Dropout-119               [-1, 128, 4]               0\n",
      "          Conv1d-120               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-121               [-1, 128, 4]               0\n",
      "      BasicBlock-122               [-1, 128, 4]               0\n",
      "     BatchNorm1d-123               [-1, 128, 4]             256\n",
      "            ReLU-124               [-1, 128, 4]               0\n",
      "          Conv1d-125               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-126               [-1, 128, 2]               0\n",
      "     BatchNorm1d-127               [-1, 128, 2]             256\n",
      "            ReLU-128               [-1, 128, 2]               0\n",
      "         Dropout-129               [-1, 128, 2]               0\n",
      "          Conv1d-130               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-131               [-1, 128, 2]               0\n",
      "       MaxPool1d-132               [-1, 128, 2]               0\n",
      "MyMaxPool1dPadSame-133               [-1, 128, 2]               0\n",
      "      BasicBlock-134               [-1, 128, 2]               0\n",
      "     BatchNorm1d-135               [-1, 128, 2]             256\n",
      "            ReLU-136               [-1, 128, 2]               0\n",
      "          Conv1d-137               [-1, 256, 2]         524,544\n",
      " MyConv1dPadSame-138               [-1, 256, 2]               0\n",
      "     BatchNorm1d-139               [-1, 256, 2]             512\n",
      "            ReLU-140               [-1, 256, 2]               0\n",
      "         Dropout-141               [-1, 256, 2]               0\n",
      "          Conv1d-142               [-1, 256, 2]       1,048,832\n",
      " MyConv1dPadSame-143               [-1, 256, 2]               0\n",
      "      BasicBlock-144               [-1, 256, 2]               0\n",
      "     BatchNorm1d-145               [-1, 256, 2]             512\n",
      "            ReLU-146               [-1, 256, 2]               0\n",
      "          Conv1d-147               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-148               [-1, 256, 1]               0\n",
      "     BatchNorm1d-149               [-1, 256, 1]             512\n",
      "            ReLU-150               [-1, 256, 1]               0\n",
      "         Dropout-151               [-1, 256, 1]               0\n",
      "          Conv1d-152               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-153               [-1, 256, 1]               0\n",
      "       MaxPool1d-154               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-155               [-1, 256, 1]               0\n",
      "      BasicBlock-156               [-1, 256, 1]               0\n",
      "     BatchNorm1d-157               [-1, 256, 1]             512\n",
      "            ReLU-158               [-1, 256, 1]               0\n",
      "          Conv1d-159               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-160               [-1, 256, 1]               0\n",
      "     BatchNorm1d-161               [-1, 256, 1]             512\n",
      "            ReLU-162               [-1, 256, 1]               0\n",
      "         Dropout-163               [-1, 256, 1]               0\n",
      "          Conv1d-164               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-165               [-1, 256, 1]               0\n",
      "      BasicBlock-166               [-1, 256, 1]               0\n",
      "     BatchNorm1d-167               [-1, 256, 1]             512\n",
      "            ReLU-168               [-1, 256, 1]               0\n",
      "          Conv1d-169               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-170               [-1, 256, 1]               0\n",
      "     BatchNorm1d-171               [-1, 256, 1]             512\n",
      "            ReLU-172               [-1, 256, 1]               0\n",
      "         Dropout-173               [-1, 256, 1]               0\n",
      "          Conv1d-174               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-175               [-1, 256, 1]               0\n",
      "       MaxPool1d-176               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-177               [-1, 256, 1]               0\n",
      "      BasicBlock-178               [-1, 256, 1]               0\n",
      "     BatchNorm1d-179               [-1, 256, 1]             512\n",
      "            ReLU-180               [-1, 256, 1]               0\n",
      "          Linear-181                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 10,465,634\n",
      "Trainable params: 10,465,634\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.27\n",
      "Params size (MB): 39.92\n",
      "Estimated Total Size (MB): 41.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=2, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "summary(model, (1, 100), device=device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecda67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m input_x, input_y = \u001b[38;5;28mtuple\u001b[39m(t.to(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[32m     28\u001b[39m pred = model(input_x)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m optimizer.zero_grad()\n\u001b[32m     31\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# train and test\n",
    "model.verbose = False\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-3, \n",
    "    weight_decay=1e-3\n",
    "    )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, # like in Hannun et al.\n",
    "    patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "    )\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "n_epoch = 50 # 100 in Hannun et al. code \n",
    "step = 0 # ?\n",
    "for _ in tqdm(range(n_epoch), desc=\"epoch\", leave=False):\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    prog_iter = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, batch in enumerate(prog_iter):\n",
    "\n",
    "        input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "        pred = model(input_x)\n",
    "        loss = loss_func(pred, input_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        writer.add_scalar('Loss/train', loss.item(), step)\n",
    "    \n",
    "    scheduler.step(_)\n",
    "                \n",
    "    # test\n",
    "    model.eval()\n",
    "    prog_iter_test = tqdm(dataloader_test, desc=\"Testing\", leave=False)\n",
    "    all_pred_prob = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(prog_iter_test):\n",
    "            input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "            pred = model(input_x)\n",
    "            all_pred_prob.append(pred.cpu().data.numpy())\n",
    "    all_pred_prob = np.concatenate(all_pred_prob)\n",
    "    all_pred = np.argmax(all_pred_prob, axis=1)\n",
    "    ## vote most common\n",
    "    final_pred = []\n",
    "    final_gt = []\n",
    "    for i_pid in np.unique(pid_test):\n",
    "        tmp_pred = all_pred[pid_test==i_pid]\n",
    "        tmp_gt = Y_test[pid_test==i_pid]\n",
    "        final_pred.append(Counter(tmp_pred).most_common(1)[0][0])\n",
    "        final_gt.append(Counter(tmp_gt).most_common(1)[0][0])\n",
    "    ## classification report\n",
    "    tmp_report = classification_report(final_gt, final_pred, output_dict=True)\n",
    "    print(confusion_matrix(final_gt, final_pred))\n",
    "    f1_score = (tmp_report['0']['f1-score'] + tmp_report['1']['f1-score'] + tmp_report['2']['f1-score'] + tmp_report['3']['f1-score'])/4\n",
    "    f1_score = (tmp_report['0']['f1-score'] + tmp_report['1']['f1-score'])/2\n",
    "    writer.add_scalar('F1/f1_score', f1_score, _)\n",
    "    writer.add_scalar('F1/label_0', tmp_report['0']['f1-score'], _)\n",
    "    writer.add_scalar('F1/label_1', tmp_report['1']['f1-score'], _)\n",
    "    writer.add_scalar('F1/label_2', tmp_report['2']['f1-score'], _)\n",
    "    writer.add_scalar('F1/label_3', tmp_report['3']['f1-score'], _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea4761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "    # The scheduler is used to change the learning rate every few \"n\" steps.\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.8))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min', \n",
    "                                                            factor=0.1, # like in Hannun et al.\n",
    "                                                            patience=2 # 2 in Hannun et al. \"two consecutive epochs\"\n",
    "                                                            )\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training stage, where we want to update the parameters.\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            ecgs, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            ecgs, labels = ecgs.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "            # Flatten the images to a vector. This is done because the classifier expects a vector as input.\n",
    "            # Could also be done by reshaping the images in the dataset.\n",
    "            # images = images.view(images.shape[0], -1)\n",
    "\n",
    "            pred = model(ecgs) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "            scheduler.step() # Update the learning rate.\n",
    "\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)),\n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                ecgs, labels = batch\n",
    "                ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "                #ecgs = ecgs.view(images.shape[0], -1)\n",
    "                pred = model(ecgs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d6ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 32, 100]             544\n",
      "   MyConv1dPadSame-2              [-1, 32, 100]               0\n",
      "       BatchNorm1d-3              [-1, 32, 100]              64\n",
      "              ReLU-4              [-1, 32, 100]               0\n",
      "            Conv1d-5              [-1, 32, 100]          16,416\n",
      "   MyConv1dPadSame-6              [-1, 32, 100]               0\n",
      "       BatchNorm1d-7              [-1, 32, 100]              64\n",
      "              ReLU-8              [-1, 32, 100]               0\n",
      "           Dropout-9              [-1, 32, 100]               0\n",
      "           Conv1d-10              [-1, 32, 100]          16,416\n",
      "  MyConv1dPadSame-11              [-1, 32, 100]               0\n",
      "       BasicBlock-12              [-1, 32, 100]               0\n",
      "      BatchNorm1d-13              [-1, 32, 100]              64\n",
      "             ReLU-14              [-1, 32, 100]               0\n",
      "           Conv1d-15               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-16               [-1, 32, 50]               0\n",
      "      BatchNorm1d-17               [-1, 32, 50]              64\n",
      "             ReLU-18               [-1, 32, 50]               0\n",
      "          Dropout-19               [-1, 32, 50]               0\n",
      "           Conv1d-20               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-21               [-1, 32, 50]               0\n",
      "        MaxPool1d-22               [-1, 32, 50]               0\n",
      "MyMaxPool1dPadSame-23               [-1, 32, 50]               0\n",
      "       BasicBlock-24               [-1, 32, 50]               0\n",
      "      BatchNorm1d-25               [-1, 32, 50]              64\n",
      "             ReLU-26               [-1, 32, 50]               0\n",
      "           Conv1d-27               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-28               [-1, 32, 50]               0\n",
      "      BatchNorm1d-29               [-1, 32, 50]              64\n",
      "             ReLU-30               [-1, 32, 50]               0\n",
      "          Dropout-31               [-1, 32, 50]               0\n",
      "           Conv1d-32               [-1, 32, 50]          16,416\n",
      "  MyConv1dPadSame-33               [-1, 32, 50]               0\n",
      "       BasicBlock-34               [-1, 32, 50]               0\n",
      "      BatchNorm1d-35               [-1, 32, 50]              64\n",
      "             ReLU-36               [-1, 32, 50]               0\n",
      "           Conv1d-37               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-38               [-1, 32, 25]               0\n",
      "      BatchNorm1d-39               [-1, 32, 25]              64\n",
      "             ReLU-40               [-1, 32, 25]               0\n",
      "          Dropout-41               [-1, 32, 25]               0\n",
      "           Conv1d-42               [-1, 32, 25]          16,416\n",
      "  MyConv1dPadSame-43               [-1, 32, 25]               0\n",
      "        MaxPool1d-44               [-1, 32, 25]               0\n",
      "MyMaxPool1dPadSame-45               [-1, 32, 25]               0\n",
      "       BasicBlock-46               [-1, 32, 25]               0\n",
      "      BatchNorm1d-47               [-1, 32, 25]              64\n",
      "             ReLU-48               [-1, 32, 25]               0\n",
      "           Conv1d-49               [-1, 64, 25]          32,832\n",
      "  MyConv1dPadSame-50               [-1, 64, 25]               0\n",
      "      BatchNorm1d-51               [-1, 64, 25]             128\n",
      "             ReLU-52               [-1, 64, 25]               0\n",
      "          Dropout-53               [-1, 64, 25]               0\n",
      "           Conv1d-54               [-1, 64, 25]          65,600\n",
      "  MyConv1dPadSame-55               [-1, 64, 25]               0\n",
      "       BasicBlock-56               [-1, 64, 25]               0\n",
      "      BatchNorm1d-57               [-1, 64, 25]             128\n",
      "             ReLU-58               [-1, 64, 25]               0\n",
      "           Conv1d-59               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-60               [-1, 64, 13]               0\n",
      "      BatchNorm1d-61               [-1, 64, 13]             128\n",
      "             ReLU-62               [-1, 64, 13]               0\n",
      "          Dropout-63               [-1, 64, 13]               0\n",
      "           Conv1d-64               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-65               [-1, 64, 13]               0\n",
      "        MaxPool1d-66               [-1, 64, 13]               0\n",
      "MyMaxPool1dPadSame-67               [-1, 64, 13]               0\n",
      "       BasicBlock-68               [-1, 64, 13]               0\n",
      "      BatchNorm1d-69               [-1, 64, 13]             128\n",
      "             ReLU-70               [-1, 64, 13]               0\n",
      "           Conv1d-71               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-72               [-1, 64, 13]               0\n",
      "      BatchNorm1d-73               [-1, 64, 13]             128\n",
      "             ReLU-74               [-1, 64, 13]               0\n",
      "          Dropout-75               [-1, 64, 13]               0\n",
      "           Conv1d-76               [-1, 64, 13]          65,600\n",
      "  MyConv1dPadSame-77               [-1, 64, 13]               0\n",
      "       BasicBlock-78               [-1, 64, 13]               0\n",
      "      BatchNorm1d-79               [-1, 64, 13]             128\n",
      "             ReLU-80               [-1, 64, 13]               0\n",
      "           Conv1d-81                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-82                [-1, 64, 7]               0\n",
      "      BatchNorm1d-83                [-1, 64, 7]             128\n",
      "             ReLU-84                [-1, 64, 7]               0\n",
      "          Dropout-85                [-1, 64, 7]               0\n",
      "           Conv1d-86                [-1, 64, 7]          65,600\n",
      "  MyConv1dPadSame-87                [-1, 64, 7]               0\n",
      "        MaxPool1d-88                [-1, 64, 7]               0\n",
      "MyMaxPool1dPadSame-89                [-1, 64, 7]               0\n",
      "       BasicBlock-90                [-1, 64, 7]               0\n",
      "      BatchNorm1d-91                [-1, 64, 7]             128\n",
      "             ReLU-92                [-1, 64, 7]               0\n",
      "           Conv1d-93               [-1, 128, 7]         131,200\n",
      "  MyConv1dPadSame-94               [-1, 128, 7]               0\n",
      "      BatchNorm1d-95               [-1, 128, 7]             256\n",
      "             ReLU-96               [-1, 128, 7]               0\n",
      "          Dropout-97               [-1, 128, 7]               0\n",
      "           Conv1d-98               [-1, 128, 7]         262,272\n",
      "  MyConv1dPadSame-99               [-1, 128, 7]               0\n",
      "      BasicBlock-100               [-1, 128, 7]               0\n",
      "     BatchNorm1d-101               [-1, 128, 7]             256\n",
      "            ReLU-102               [-1, 128, 7]               0\n",
      "          Conv1d-103               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-104               [-1, 128, 4]               0\n",
      "     BatchNorm1d-105               [-1, 128, 4]             256\n",
      "            ReLU-106               [-1, 128, 4]               0\n",
      "         Dropout-107               [-1, 128, 4]               0\n",
      "          Conv1d-108               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-109               [-1, 128, 4]               0\n",
      "       MaxPool1d-110               [-1, 128, 4]               0\n",
      "MyMaxPool1dPadSame-111               [-1, 128, 4]               0\n",
      "      BasicBlock-112               [-1, 128, 4]               0\n",
      "     BatchNorm1d-113               [-1, 128, 4]             256\n",
      "            ReLU-114               [-1, 128, 4]               0\n",
      "          Conv1d-115               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-116               [-1, 128, 4]               0\n",
      "     BatchNorm1d-117               [-1, 128, 4]             256\n",
      "            ReLU-118               [-1, 128, 4]               0\n",
      "         Dropout-119               [-1, 128, 4]               0\n",
      "          Conv1d-120               [-1, 128, 4]         262,272\n",
      " MyConv1dPadSame-121               [-1, 128, 4]               0\n",
      "      BasicBlock-122               [-1, 128, 4]               0\n",
      "     BatchNorm1d-123               [-1, 128, 4]             256\n",
      "            ReLU-124               [-1, 128, 4]               0\n",
      "          Conv1d-125               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-126               [-1, 128, 2]               0\n",
      "     BatchNorm1d-127               [-1, 128, 2]             256\n",
      "            ReLU-128               [-1, 128, 2]               0\n",
      "         Dropout-129               [-1, 128, 2]               0\n",
      "          Conv1d-130               [-1, 128, 2]         262,272\n",
      " MyConv1dPadSame-131               [-1, 128, 2]               0\n",
      "       MaxPool1d-132               [-1, 128, 2]               0\n",
      "MyMaxPool1dPadSame-133               [-1, 128, 2]               0\n",
      "      BasicBlock-134               [-1, 128, 2]               0\n",
      "     BatchNorm1d-135               [-1, 128, 2]             256\n",
      "            ReLU-136               [-1, 128, 2]               0\n",
      "          Conv1d-137               [-1, 256, 2]         524,544\n",
      " MyConv1dPadSame-138               [-1, 256, 2]               0\n",
      "     BatchNorm1d-139               [-1, 256, 2]             512\n",
      "            ReLU-140               [-1, 256, 2]               0\n",
      "         Dropout-141               [-1, 256, 2]               0\n",
      "          Conv1d-142               [-1, 256, 2]       1,048,832\n",
      " MyConv1dPadSame-143               [-1, 256, 2]               0\n",
      "      BasicBlock-144               [-1, 256, 2]               0\n",
      "     BatchNorm1d-145               [-1, 256, 2]             512\n",
      "            ReLU-146               [-1, 256, 2]               0\n",
      "          Conv1d-147               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-148               [-1, 256, 1]               0\n",
      "     BatchNorm1d-149               [-1, 256, 1]             512\n",
      "            ReLU-150               [-1, 256, 1]               0\n",
      "         Dropout-151               [-1, 256, 1]               0\n",
      "          Conv1d-152               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-153               [-1, 256, 1]               0\n",
      "       MaxPool1d-154               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-155               [-1, 256, 1]               0\n",
      "      BasicBlock-156               [-1, 256, 1]               0\n",
      "     BatchNorm1d-157               [-1, 256, 1]             512\n",
      "            ReLU-158               [-1, 256, 1]               0\n",
      "          Conv1d-159               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-160               [-1, 256, 1]               0\n",
      "     BatchNorm1d-161               [-1, 256, 1]             512\n",
      "            ReLU-162               [-1, 256, 1]               0\n",
      "         Dropout-163               [-1, 256, 1]               0\n",
      "          Conv1d-164               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-165               [-1, 256, 1]               0\n",
      "      BasicBlock-166               [-1, 256, 1]               0\n",
      "     BatchNorm1d-167               [-1, 256, 1]             512\n",
      "            ReLU-168               [-1, 256, 1]               0\n",
      "          Conv1d-169               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-170               [-1, 256, 1]               0\n",
      "     BatchNorm1d-171               [-1, 256, 1]             512\n",
      "            ReLU-172               [-1, 256, 1]               0\n",
      "         Dropout-173               [-1, 256, 1]               0\n",
      "          Conv1d-174               [-1, 256, 1]       1,048,832\n",
      " MyConv1dPadSame-175               [-1, 256, 1]               0\n",
      "       MaxPool1d-176               [-1, 256, 1]               0\n",
      "MyMaxPool1dPadSame-177               [-1, 256, 1]               0\n",
      "      BasicBlock-178               [-1, 256, 1]               0\n",
      "     BatchNorm1d-179               [-1, 256, 1]             512\n",
      "            ReLU-180               [-1, 256, 1]               0\n",
      "          Conv1d-181                 [-1, 2, 1]             514\n",
      "================================================================\n",
      "Total params: 10,465,634\n",
      "Trainable params: 10,465,634\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.27\n",
      "Params size (MB): 39.92\n",
      "Estimated Total Size (MB): 41.19\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]:   0%|                                                                                                  | 0/240 [00:02<?, ?it/s]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1f3621dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/python/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/python/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1618, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/python/lib/python3.13/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/python/lib/python3.13/multiprocessing/popen_fork.py\", line 41, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/python/lib/python3.13/multiprocessing/connection.py\", line 1148, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/python/lib/python3.13/selectors.py\", line 398, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m summary(model, (\u001b[32m1\u001b[39m,\u001b[32m100\u001b[39m), device=device_str)\n\u001b[32m     44\u001b[39m model.verbose = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinished training!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, loss_func, tb_logger, epochs, name)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Flatten the images to a vector. This is done because the classifier expects a vector as input.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Could also be done by reshaping the images in the dataset.\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# images = images.view(images.shape[0], -1)\u001b[39;00m\n\u001b[32m     38\u001b[39m pred = model(ecgs) \u001b[38;5;66;03m# Stage 1: Forward().\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Compute the loss over the predictions and the ground truth.\u001b[39;00m\n\u001b[32m     40\u001b[39m loss.backward()  \u001b[38;5;66;03m# Stage 2: Backward().\u001b[39;00m\n\u001b[32m     41\u001b[39m optimizer.step() \u001b[38;5;66;03m# Stage 3: Update the parameters.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Create a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = \"logs\"\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "\n",
    "# make model\n",
    "device_str = \"cuda\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "kernel_size = 16 # 16 in Hannun et al.\n",
    "stride = 2\n",
    "n_block = 16 # 16 in Hannun et al.\n",
    "downsample_gap = 2 # 2 in Hannun et al.\n",
    "increasefilter_gap = 4 # 4 in Hannun et al.\n",
    "\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=32, # 32 in Hannun et al.\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    groups=1, # like a classical ResNet\n",
    "    n_block=n_block, \n",
    "    n_classes=2, \n",
    "    downsample_gap=downsample_gap, \n",
    "    increasefilter_gap=increasefilter_gap, \n",
    "    use_bn=True,\n",
    "    use_do=True\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "summary(model, (1,100), device=device_str)\n",
    "\n",
    "model.verbose = False\n",
    "\n",
    "train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=epochs, name=\"Default\")\n",
    "\n",
    "print()\n",
    "print(\"Finished training!\")\n",
    "print(\"How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\")\n",
    "print(f\"Training Acc: {model.getTestAcc(labled_train_loader)[1] * 100}%\")\n",
    "print(f\"Validation Acc: {model.getTestAcc(labled_val_loader)[1] * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
